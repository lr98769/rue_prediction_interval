{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f113d1",
   "metadata": {},
   "source": [
    "# S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib.gridspec import SubplotSpec\n",
    "import joblib\n",
    "import gpflow\n",
    "\n",
    "# KNN\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# Cond Gaussian\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Weighted\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import weightedcalcs as wc\n",
    "\n",
    "# Gaussian Copula\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, truncnorm\n",
    "\n",
    "# BNN\n",
    "import torchbnn as bnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "\n",
    "# DER\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning import LightningDataModule\n",
    "from torch_uncertainty.models.mlp import mlp\n",
    "from torch_uncertainty.losses import DERLoss\n",
    "from torch_uncertainty.routines import RegressionRoutine\n",
    "from torch_uncertainty.layers.distributions import NormalInverseGammaLayer\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "import contextlib\n",
    "\n",
    "data_label = \"snp500\"\n",
    "seed = 2023\n",
    "\n",
    "# File paths\n",
    "fp_notebooks_folder = \"./\"\n",
    "fp_code_folder = \"../\"\n",
    "fp_processed_folder = os.path.join(fp_code_folder, \"../processed_data\")\n",
    "fp_downsampled_folder = os.path.join(fp_processed_folder, \"downsampled\")\n",
    "fp_downsampled_dropna_file = os.path.join(fp_downsampled_folder, \"dropna.csv\")\n",
    "fp_downsampled_scaler_file = os.path.join(fp_downsampled_folder, \"scaler.pkl\")\n",
    "\n",
    "fp_project_checkpoints = os.path.join(fp_code_folder, \"checkpoints\", data_label)\n",
    "fp_tuning = os.path.join(fp_project_checkpoints, \"tuning\")\n",
    "fp_project_models = os.path.join(fp_project_checkpoints, \"models\")\n",
    "fp_project_predictions = os.path.join(fp_project_checkpoints, \"predictions\")\n",
    "fp_project_pi_predictions = os.path.join(fp_project_checkpoints, \"pi_predictions\")\n",
    "fp_project_model_evaluations = os.path.join(fp_project_checkpoints, \"model_evaluation\")\n",
    "fp_project_consolidated_results = os.path.join(fp_project_checkpoints, \"consolidated_results\")\n",
    "fp_time_log = os.path.join(fp_project_consolidated_results, \"runtime.log\")\n",
    "\n",
    "def create_folder(fp):\n",
    "    if not os.path.exists(fp):\n",
    "        os.makedirs(fp)\n",
    "        return True\n",
    "    else:\n",
    "        False\n",
    "\n",
    "def create_all_seed_folders(cur_seed):\n",
    "    fp_checkpoint_folders = [fp_project_models, fp_tuning, fp_project_predictions, fp_project_model_evaluations, fp_project_pi_predictions]\n",
    "    for fp_folder in fp_checkpoint_folders:\n",
    "        fp = os.path.join(fp_folder, str(cur_seed))\n",
    "        create_folder(fp)\n",
    "    print(f\"All folders created for seed = {cur_seed}!\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create all folders\n",
    "create_all_seed_folders(seed)\n",
    "create_folder(fp_project_consolidated_results)\n",
    "\n",
    "# Check GPU is available\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# function to show df\n",
    "def display_df(df):\n",
    "    display(df.head())\n",
    "    print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ad2a9",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fp_downsampled_dropna_file, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769cd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[:60].to_list()\n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols_1 = [col for col in df.columns if \"PredMin1\" in col]\n",
    "pred_cols_2 = [col for col in df.columns if \"PredMin2\" in col]\n",
    "pred_cols_3 = [col for col in df.columns if \"PredMin3\" in col]\n",
    "print(pred_cols_1)\n",
    "print(pred_cols_2)\n",
    "print(pred_cols_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c339db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train, validation and test sets\n",
    "def train_valid_test_split(df, pred_cols):\n",
    "    df_train, df_valid, df_test = df[df[\"train\"]], df[df[\"valid\"]], df[df[\"test\"]]\n",
    "    num_pred_cols = len(pred_cols)\n",
    "    \n",
    "    # Plot distribution of pred_col for each set\n",
    "    fig, axes = plt.subplots(num_pred_cols, 3, figsize=(10, 2*num_pred_cols))\n",
    "    for i, col in enumerate(pred_cols):\n",
    "        axes[i, 0].hist(df_train[col])\n",
    "        axes[i, 0].set_xlabel(\"Train\")\n",
    "        axes[i, 0].set_ylabel(col.split(\"_\")[0])\n",
    "        axes[i, 1].hist(df_valid[col])\n",
    "        axes[i, 1].set_xlabel(\"Valid\")\n",
    "        axes[i, 2].hist(df_test[col])\n",
    "        axes[i, 2].set_xlabel(\"Test\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "df_train_1, df_valid_1, df_test_1 = train_valid_test_split(df, pred_cols=pred_cols_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af25c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2, df_valid_2, df_test_2 = train_valid_test_split(df, pred_cols=pred_cols_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597715d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_3, df_valid_3, df_test_3 = train_valid_test_split(df, pred_cols=pred_cols_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91fb28-8068-4be6-a3e6-e036a65019ee",
   "metadata": {},
   "source": [
    "## Timer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c57ae-2ecd-42e4-8382-3220969ba4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def restart_timer(self):\n",
    "        self.description = None\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.duration = None\n",
    "        \n",
    "    def start(self, description):\n",
    "        self.restart_timer()\n",
    "        self.description = description\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def end(self):\n",
    "        self.end_time = time.time()\n",
    "        self.duration = self.end_time-self.start_time\n",
    "        print(f\"{self.description} took {self.duration}s. \")\n",
    "        self._log_time()\n",
    "\n",
    "    def _log_time(self):\n",
    "        with open(fp_time_log, \"a+\") as myfile:\n",
    "            myfile.write(f\"{self.seed},{self.description},{self.duration}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f4abc-5468-4b4a-8f98-4d7060bf20e6",
   "metadata": {},
   "source": [
    "## RUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def display_history(history, show_acc=False):\n",
    "    if show_acc:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 2.5))\n",
    "        axes[0].plot(history.history['loss'])\n",
    "        axes[0].plot(history.history['val_loss'])\n",
    "        axes[0].set_title('Model Loss')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].legend(['Train', 'Val'], loc='upper left')\n",
    "        axes[1].plot(history.history['accuracy'])\n",
    "        axes[1].plot(history.history['val_accuracy'])\n",
    "        axes[1].set_title('Model Accuracy')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].legend(['Train', 'Val'], loc='upper left')\n",
    "        axes[2].plot(history.history['f1_score'])\n",
    "        axes[2].plot(history.history['val_f1_score'])\n",
    "        axes[2].set_title('Model F1 Score')\n",
    "        axes[2].set_ylabel('F1 Score')\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].legend(['Train', 'Val'], loc='upper left')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(3, 2.5))\n",
    "        axes.plot(history.history['loss'])\n",
    "        axes.plot(history.history['val_loss'])\n",
    "        axes.set_title('Model Loss')\n",
    "        axes.set_ylabel('Loss')\n",
    "        axes.set_xlabel('Epoch')\n",
    "        axes.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "class AE_Regressor:\n",
    "    def __init__(self, encoder_width, encoder_depth, decoder_width, decoder_depth, predictors, output_features):\n",
    "        from keras.layers import Dense, Dropout\n",
    "        self.predictors = predictors\n",
    "        self.output_features = output_features\n",
    "        self.num_predictors = len(self.predictors)\n",
    "        self.num_outputs = len(self.output_features)\n",
    "        self.encoder_width = encoder_width\n",
    "        self.encoder_depth = encoder_depth\n",
    "        self.decoder_width = decoder_width\n",
    "        self.decoder_depth = decoder_depth\n",
    "        \n",
    "        # Instantiate model layers\n",
    "        self.inputs = tf.keras.Input(shape=(self.num_predictors,))\n",
    "        self.encoder = tf.keras.Sequential(list(\n",
    "            Dense(self.encoder_width, \"relu\") for i in range(self.encoder_depth) # , kernel_regularizer=\"l2\"\n",
    "        ), name=\"encoder\")\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            Dense(self.decoder_width, \"relu\" , kernel_regularizer=\"l2\") for i in range(self.decoder_depth-1) # , kernel_regularizer=\"l2\"\n",
    "        ]+[Dense(self.num_predictors)], name=\"decoder\")\n",
    "        self.regressor = tf.keras.Sequential([\n",
    "            Dropout(rate=0.2),\n",
    "            Dense(self.num_outputs)\n",
    "        ], name=\"regressor\")\n",
    "    \n",
    "    # Smote is external\n",
    "    def train_regressor(\n",
    "        self, train_X, train_y, val_X, val_y, batch_size, max_epochs, verbose, patience):\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        # Define regressor\n",
    "        pred = self.encoder(self.inputs)\n",
    "        pred = self.regressor(pred)\n",
    "        self.predictor = tf.keras.Model(inputs=self.inputs, outputs=pred, name=\"regression_model\")\n",
    "        # Train predictor\n",
    "        self.predictor.compile(\n",
    "            loss=\"mse\",\n",
    "            optimizer=tf.keras.optimizers.Adam()\n",
    "        )\n",
    "        es = EarlyStopping(\n",
    "            monitor='val_loss', mode='min', verbose=1, patience=patience, restore_best_weights=True)\n",
    "        self.predictor_history = self.predictor.fit(\n",
    "            train_X, train_y, \n",
    "            epochs=max_epochs, \n",
    "            validation_data=(val_X, val_y),\n",
    "            verbose=verbose,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es],\n",
    "        )\n",
    "        print(\"- Regressor Training History\")\n",
    "        display_history(self.predictor_history)\n",
    "        best_index = np.argmin(self.predictor_history.history['val_loss'])\n",
    "        return (\n",
    "            self.predictor_history.history['val_loss'][best_index], \n",
    "            best_index\n",
    "        )\n",
    "        \n",
    "    def train_decoder(\n",
    "        self, train_X, val_X, batch_size, max_epochs, verbose, patience):\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        # Define AE\n",
    "        self.encoder.trainable=False # Freeze weights\n",
    "        x = self.encoder(self.inputs)\n",
    "        x = self.decoder(x)\n",
    "        self.ae = tf.keras.Model(inputs=self.inputs, outputs=x, name=\"ae_model\")\n",
    "        # Train AE\n",
    "        self.ae.compile(\n",
    "            loss=\"mse\",\n",
    "            optimizer=tf.keras.optimizers.Adam()\n",
    "        )\n",
    "        es = EarlyStopping(\n",
    "            monitor='val_loss', mode='min', verbose=1, patience=patience, restore_best_weights=True)\n",
    "        self.ae_history = self.ae.fit(\n",
    "            train_X, train_X, \n",
    "            epochs=max_epochs, \n",
    "            validation_data=(val_X, val_X),\n",
    "            verbose=verbose,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "        print(\"- Autoencoder Training History\")\n",
    "        display_history(self.ae_history)\n",
    "        best_epoch = np.argmin(self.ae_history.history['val_loss'])\n",
    "        return self.ae_history.history['val_loss'][best_epoch], best_epoch\n",
    "\n",
    "    def replace_encoder_predictor(self, prev_model):\n",
    "        self.encoder = prev_model.encoder\n",
    "        self.regressor = prev_model.regressor\n",
    "        self.encoder_width = prev_model.encoder_width\n",
    "        self.encoder_depth = prev_model.encoder_depth\n",
    "\n",
    "    def predict(self, inputs, with_mae=True, weighted=False, dropout_activated=False):\n",
    "        # Encode\n",
    "        encoder_output = self.encoder(inputs)\n",
    "        \n",
    "        # Get forecast result\n",
    "        regressor_output = self.regressor(encoder_output, training=dropout_activated)\n",
    "        if with_mae:\n",
    "            # Reconstruct\n",
    "            decoder_output = self.decoder(encoder_output)\n",
    "            return regressor_output, decoder_output\n",
    "        else:\n",
    "            return regressor_output\n",
    "\n",
    "    def get_config(self):\n",
    "        return dict(\n",
    "            encoder_width=self.encoder_width, \n",
    "            encoder_depth=self.encoder_depth, \n",
    "            decoder_width=self.decoder_width, \n",
    "            decoder_depth=self.decoder_depth, \n",
    "            predictors=self.predictors, \n",
    "            output_features=self.output_features\n",
    "        )\n",
    "        \n",
    "def save_model(model, name, fp_checkpoints, override=False):\n",
    "    import pickle\n",
    "    model_folder = os.path.join(fp_checkpoints, name)\n",
    "    if os.path.exists(model_folder):\n",
    "        print(\"Model checkpoint already exists!\")\n",
    "        if not override:\n",
    "            return\n",
    "    else:\n",
    "        os.makedirs(model_folder)\n",
    "    \n",
    "    # Save Parameters\n",
    "    parameters_to_save = model.get_config()\n",
    "    parameter_filename = os.path.join(fp_checkpoints, name, \"parameters.pickle\")\n",
    "    with open(parameter_filename, 'wb') as handle:\n",
    "        pickle.dump(parameters_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Save Model\n",
    "    inputs = model.inputs\n",
    "    encoder_output = model.encoder(inputs)\n",
    "    decoder_output = model.decoder(encoder_output)\n",
    "    regressor_output = model.regressor(encoder_output)\n",
    "    model = tf.keras.Model(inputs, [regressor_output, decoder_output])\n",
    "    model_filename = os.path.join(fp_checkpoints, name, \"model.h5\")\n",
    "    model.save(model_filename)\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "def load_model(name, fp_checkpoints): \n",
    "    import pickle\n",
    "    model_folder = os.path.join(fp_checkpoints, name)\n",
    "    parameter_filename = os.path.join(fp_checkpoints, name, \"parameters.pickle\")\n",
    "    model_filename = os.path.join(fp_checkpoints, name, \"model.h5\")\n",
    "    \n",
    "    if not os.path.exists(model_folder):\n",
    "        print(\"model checkpoint does not exist!\")\n",
    "        return\n",
    "    model = tf.keras.models.load_model(model_filename)\n",
    "    with open(parameter_filename, 'rb') as handle:\n",
    "        parameters = pickle.load(handle)\n",
    "    \n",
    "    ae_regressor = AE_Regressor(**parameters)\n",
    "    ae_regressor.encoder = model.get_layer(\"encoder\")\n",
    "    ae_regressor.decoder = model.get_layer(\"decoder\")\n",
    "    ae_regressor.regressor = model.get_layer(\"regressor\")\n",
    "    \n",
    "    return ae_regressor\n",
    "\n",
    "def model_tuning_regressor(\n",
    "    param_grid, predictors, pred_cols, train_df, valid_df, seed,\n",
    "    batch_size=128, max_epochs=5000, verbose=1, patience=10):\n",
    "    train_X, train_y = (\n",
    "        train_df[predictors].values.astype('float32'), train_df[pred_cols].values.astype('float32'))\n",
    "    valid_X, valid_y = (\n",
    "        valid_df[predictors].values.astype('float32'), valid_df[pred_cols].values.astype('float32'))\n",
    "    loss_list = []\n",
    "    epoch_list = []\n",
    "    time_spent_list = []\n",
    "    tuning_df_list = []\n",
    "    parameter_list = list(ParameterGrid(param_grid))\n",
    "    pbar = tqdm(parameter_list)\n",
    "    for param_dict in pbar:\n",
    "        pbar.set_description(f\"{param_dict}\")\n",
    "        set_seed(seed)\n",
    "        regressor = AE_Regressor(**param_dict, predictors=predictors, output_features=pred_cols)\n",
    "        start = time.time()\n",
    "        val_loss, best_epoch = regressor.train_regressor(\n",
    "            train_X, train_y, valid_X, valid_y, batch_size, max_epochs, verbose, patience)\n",
    "        cur_param_dict = param_dict.copy()\n",
    "        cur_param_dict[\"loss\"] = val_loss\n",
    "        cur_param_dict[\"epochs\"] = best_epoch\n",
    "        cur_param_dict[\"time/s\"] = time.time()-start\n",
    "        tuning_df_list.append(cur_param_dict)\n",
    "    tuning_df = pd.DataFrame(tuning_df_list)\n",
    "    best_param_idx = tuning_df['loss'].idxmin()\n",
    "    tuning_df[\"best_hyperparameter\"] = [True if i==best_param_idx else False for i in range(len(tuning_df))]\n",
    "    best_param = parameter_list[best_param_idx]\n",
    "    return tuning_df, best_param\n",
    "\n",
    "def get_model_error_corr(predictor, x, y):\n",
    "    y_pred, x_pred = predictor.predict(x)\n",
    "    rue = np.mean(np.abs(x-x_pred), axis=-1)\n",
    "    mae = np.mean(np.abs(y-y_pred), axis=-1)\n",
    "    corr, pvalue = pearsonr(mae, rue)\n",
    "    return corr\n",
    "\n",
    "def model_tuning_decoder(\n",
    "    param_grid, predictors, pred_cols, train_df, valid_df, seed, prev_model,\n",
    "    atch_size=128, max_epochs=5000, verbose=1, patience=10):\n",
    "    train_X, train_y = (\n",
    "        train_df[predictors].values.astype('float32'), train_df[pred_cols].values.astype('float32'))\n",
    "    valid_X, valid_y = (\n",
    "        valid_df[predictors].values.astype('float32'), valid_df[pred_cols].values.astype('float32'))\n",
    "    tuning_df_list = []\n",
    "    parameter_list = list(ParameterGrid(param_grid))\n",
    "    best_corr = -np.inf\n",
    "    pbar = tqdm(parameter_list)\n",
    "    for param_dict in pbar:\n",
    "        pbar.set_description(f\"{param_dict}, best corr: {best_corr:.5f}\")\n",
    "        set_seed(seed)\n",
    "        predictor = AE_Regressor(**param_dict, predictors=predictors, output_features=pred_cols)\n",
    "        predictor.replace_encoder_predictor(prev_model)\n",
    "        start = time.time()\n",
    "        val_loss, best_epoch = predictor.train_decoder(\n",
    "            train_X, valid_X, batch_size, max_epochs, verbose, patience)\n",
    "        timing = time.time()-start\n",
    "        corr = get_model_error_corr(predictor, valid_X, valid_y)\n",
    "        cur_param_dict = param_dict.copy()\n",
    "        cur_param_dict[\"loss\"] = val_loss\n",
    "        cur_param_dict[\"corr\"] = corr\n",
    "        cur_param_dict[\"epochs\"] = best_epoch\n",
    "        cur_param_dict[\"time/s\"] = timing\n",
    "        tuning_df_list.append(cur_param_dict)\n",
    "        best_corr = max(corr, best_corr)\n",
    "    tuning_df = pd.DataFrame(tuning_df_list)\n",
    "    best_param_idx = tuning_df[\"corr\"].idxmax()\n",
    "    tuning_df[\"best_hyperparameter\"] = [True if i==best_param_idx else False for i in range(len(tuning_df))]\n",
    "    best_param = parameter_list[best_param_idx]\n",
    "    return tuning_df, best_param\n",
    "\n",
    "def model_training(\n",
    "    hp_dict, predictors, pred_cols, train_df, valid_df, seed,\n",
    "    batch_size=128, max_epochs=5000, verbose=1, patience=10):\n",
    "    set_seed(seed)\n",
    "    timer = Timer(seed)\n",
    "    \n",
    "    # Get data\n",
    "    train_X, train_y = (\n",
    "        train_df[predictors].values.astype('float32'), train_df[pred_cols].values.astype('float32'))\n",
    "    valid_X, valid_y = (\n",
    "        valid_df[predictors].values.astype('float32'), valid_df[pred_cols].values.astype('float32'))\n",
    "    \n",
    "    # Train Regressor\n",
    "    ae_regressor = AE_Regressor(**hp_dict, predictors=predictors, output_features=pred_cols)\n",
    "    timer.start(description=\"Training Encoder and Predictor\")\n",
    "    valid_loss_regressor, best_epoch = ae_regressor.train_regressor(\n",
    "        train_X, train_y, valid_X, valid_y, \n",
    "        batch_size, max_epochs, verbose, patience)\n",
    "    timer.end()\n",
    "    \n",
    "    # Train decoder\n",
    "    timer.start(description=\"Training Decoder\")\n",
    "    valid_loss_ae = ae_regressor.train_decoder(\n",
    "        train_X, valid_X, batch_size, max_epochs, verbose, patience)\n",
    "    timer.end()\n",
    "    return ae_regressor\n",
    "\n",
    "def model_training_predictor(\n",
    "    hp_dict, predictors, pred_cols, train_df, valid_df, seed,\n",
    "    batch_size=128, max_epochs=5000, verbose=1, patience=10):\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Get data\n",
    "    train_X, train_y = (\n",
    "        train_df[predictors].values.astype('float32'), train_df[pred_cols].values.astype('float32'))\n",
    "    valid_X, valid_y = (\n",
    "        valid_df[predictors].values.astype('float32'), valid_df[pred_cols].values.astype('float32'))\n",
    "    \n",
    "    # Train Regressor\n",
    "    ae_regressor = AE_Regressor(**hp_dict, predictors=predictors, output_features=pred_cols)\n",
    "    \n",
    "    start = time.time()\n",
    "    # Train classifier\n",
    "    valid_loss_regressor, best_epoch = ae_regressor.train_regressor(\n",
    "        train_X, train_y, valid_X, valid_y, \n",
    "        batch_size, max_epochs, verbose, patience)\n",
    "    \n",
    "    # Train decoder\n",
    "    # valid_loss_ae = ae_regressor.train_decoder(\n",
    "    #     train_X, valid_X, batch_size, max_epochs, verbose, patience)\n",
    "    \n",
    "    print(f\"Training took {time.time()-start}s.\")\n",
    "    return ae_regressor\n",
    "\n",
    "def model_training_decoder(\n",
    "    hp_dict, predictors, pred_cols, train_df, valid_df, seed, prev_model, \n",
    "    batch_size=128, max_epochs=5000, verbose=1, patience=10):\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Get data\n",
    "    train_X, train_y = (\n",
    "        train_df[predictors].values.astype('float32'), train_df[pred_cols].values.astype('float32'))\n",
    "    valid_X, valid_y = (\n",
    "        valid_df[predictors].values.astype('float32'), valid_df[pred_cols].values.astype('float32'))\n",
    "    \n",
    "    # Train Regressor\n",
    "    ae_regressor = AE_Regressor(**hp_dict, predictors=predictors, output_features=pred_cols)\n",
    "    ae_regressor.replace_encoder_predictor(prev_model)\n",
    "    \n",
    "    start = time.time()\n",
    "    # Train classifier\n",
    "    # valid_loss_regressor, best_epoch = ae_regressor.train_regressor(\n",
    "    #     train_X, train_y, valid_X, valid_y, \n",
    "    #     batch_size, max_epochs, verbose, patience)\n",
    "    \n",
    "    # Train decoder\n",
    "    valid_loss_ae = ae_regressor.train_decoder(\n",
    "        train_X, valid_X, batch_size, max_epochs, verbose, patience)\n",
    "    \n",
    "    print(f\"Training took {time.time()-start}s.\")\n",
    "    return ae_regressor\n",
    "\n",
    "def model_test_predictions(\n",
    "    ae_regressor, df_train, df_test, pred_cols, predictors, regressor_label, pred_min, T=10, seed=seed):\n",
    "    df_test = df_test.copy()\n",
    "    set_seed(seed)\n",
    "    timer = Timer(seed)\n",
    "    test_X, test_y = (\n",
    "        df_test[predictors].values.astype('float32'), df_test[pred_cols].values.astype('float32'))\n",
    "    \n",
    "    # MAE\n",
    "    timer.start(description=\"Predicting with Decoder + Predictor\")\n",
    "    test_y_pred, test_X_reconstruction = ae_regressor.predict(inputs=test_X)\n",
    "    timer.end()\n",
    "    # - Add to df\n",
    "    predicted_colnames = [col+\"_direct\"+regressor_label for col in pred_cols]\n",
    "    reconstruction_colnames = [col+\"_reconstruction\"+regressor_label for col in predictors]\n",
    "    rue_colname = \"rue\"\n",
    "    df_test[predicted_colnames] = test_y_pred\n",
    "    df_test[reconstruction_colnames] = test_X_reconstruction\n",
    "    df_test[rue_colname] = np.mean(np.abs(test_X_reconstruction-test_X), axis=1)\n",
    "    \n",
    "    # For MC Dropout\n",
    "    # - Sample with dropout\n",
    "    timer.start(description=\"Predicting with MC Dropout\")\n",
    "    test_y_sample_preds = []\n",
    "    for i in range(T):\n",
    "        test_y_pred = ae_regressor.predict(inputs=test_X, dropout_activated=True, with_mae=False)\n",
    "        test_y_sample_preds.append(test_y_pred)\n",
    "    timer.end()\n",
    "    test_y_sample_preds = np.array(test_y_sample_preds)\n",
    "    test_y_sample_preds =  test_y_sample_preds.transpose((1, 2, 0))\n",
    "    # - Get mean and std of all sample predictions\n",
    "    test_y_mean_pred = np.mean(test_y_sample_preds, axis=-1)\n",
    "    test_y_std_pred = np.std(test_y_sample_preds, axis=-1, ddof=1)\n",
    "\n",
    "    # - Add to df\n",
    "    predicted_mean_colnames = [col+\"_mean\"+regressor_label for col in pred_cols]\n",
    "    predicted_std_colnames = [col+\"_std\"+regressor_label for col in pred_cols]\n",
    "    mcd_colname = \"mcd\"\n",
    "    df_test[predicted_mean_colnames] = test_y_mean_pred\n",
    "    df_test[predicted_std_colnames] = test_y_std_pred\n",
    "    df_test[mcd_colname] = np.mean(test_y_std_pred, axis=-1)\n",
    "    \n",
    "    if df_test['target_index'].dtype != \"int64\":\n",
    "        df_test['target_index'] = df_test['target_index'].apply(lambda x: eval(x)[pred_min-1])\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017f2b8",
   "metadata": {},
   "source": [
    "### Tune and Train Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845611fc-6fe7-4c86-99f4-cea568e1fda4",
   "metadata": {},
   "source": [
    "#### Tune Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73440884-54b2-49d2-9dcc-8feb7a09a21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rue_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "# all_rue_best_hp = {}\n",
    "for time_label, time_info_dict in tqdm(rue_dict.items(), total=len(rue_dict)):\n",
    "    rue_tuning_df, rue_best_hp = model_tuning_regressor(\n",
    "        param_grid=dict(\n",
    "            encoder_width = [256, 512, 1028], # , 256, 512\n",
    "            encoder_depth = [1, 2, 3], #  3, 4\n",
    "            decoder_width = [128],\n",
    "            decoder_depth = [2]\n",
    "        ), predictors=predictors, pred_cols=time_info_dict[\"outputs\"], \n",
    "        train_df=time_info_dict[\"train_df\"], valid_df=time_info_dict[\"valid_df\"], seed=seed,\n",
    "        batch_size=batch_size, max_epochs=10000, verbose=1, patience=20\n",
    "    )\n",
    "    display(rue_tuning_df)\n",
    "    rue_tuning_df.to_csv(os.path.join(fp_tuning, str(seed), f\"tuning_rue_{time_label}.csv\"))\n",
    "    all_rue_best_hp[time_label] = rue_best_hp\n",
    "joblib.dump(all_rue_best_hp, os.path.join(fp_tuning, str(seed), \"all_rue_predictor_best_hp.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8089a-089a-43e1-ba95-d7c0448613de",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rue_predictor_best_hp = joblib.load(os.path.join(fp_tuning, str(seed), \"all_rue_predictor_best_hp.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a75a1bf-44f7-449a-a13c-bcf2ea611aae",
   "metadata": {},
   "source": [
    "#### Train Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a6483-e9ba-42c2-834e-c49de1057a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rue_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "all_rue_decoder_best_hp = {}\n",
    "for time_label, time_info_dict in tqdm(rue_dict.items(), total=len(rue_dict)):\n",
    "    best_predictor_hp = all_rue_predictor_best_hp[time_label]\n",
    "    ae_regressor = model_training_predictor(\n",
    "        best_predictor_hp, predictors=predictors, pred_cols=time_info_dict[\"outputs\"], \n",
    "        train_df=time_info_dict[\"train_df\"], valid_df = time_info_dict[\"valid_df\"], seed=seed,\n",
    "        batch_size=batch_size, max_epochs=10000, verbose=1, patience=20\n",
    "    ) \n",
    "    save_model(model=ae_regressor, name=f\"rue_predictor_{time_label}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6d3a1-2e05-4bda-a156-3ff0c51e6097",
   "metadata": {},
   "source": [
    "### Tune and Train Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c83cd-3baa-4331-bd0e-4c91b94395e6",
   "metadata": {},
   "source": [
    "#### Tune Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03841faa-247b-476f-8c88-d6faead5e111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rue_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "all_rue_decoder_best_hp = {}\n",
    "for time_label, time_info_dict in tqdm(rue_dict.items(), total=len(rue_dict)):\n",
    "    best_predictor_hp = all_rue_predictor_best_hp[time_label]\n",
    "    prev_model = load_model(name=f\"rue_predictor_{time_label}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "    rue_tuning_df, rue_best_hp = model_tuning_decoder(\n",
    "        param_grid=dict(\n",
    "            encoder_width = [best_predictor_hp[\"encoder_width\"]], # , 256, 512\n",
    "            encoder_depth = [best_predictor_hp[\"encoder_depth\"]], #  3, 4\n",
    "            decoder_width = [256, 512, 1028], \n",
    "            decoder_depth = [1, 2, 3]\n",
    "        ), predictors=predictors, pred_cols=time_info_dict[\"outputs\"], \n",
    "        train_df=time_info_dict[\"train_df\"], valid_df=time_info_dict[\"valid_df\"], seed=seed,\n",
    "        max_epochs=10000, verbose=1, patience=20, prev_model=prev_model\n",
    "    )\n",
    "    display(rue_tuning_df)\n",
    "    rue_tuning_df.to_csv(os.path.join(fp_tuning, str(seed), f\"tuning_rue_decoder_{time_label}.csv\"))\n",
    "    all_rue_decoder_best_hp[time_label] = rue_best_hp\n",
    "joblib.dump(all_rue_decoder_best_hp, os.path.join(fp_tuning, str(seed), \"all_rue_decoder_best_hp.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ac100-355f-489c-aff6-400220066246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Regularisation (L2=0.01) and MSE: 0.33679\n",
    "# With Regularisation and MAE: 0.296979\n",
    "# Without Regularisation and MSE: 0.274257\n",
    "# With Regularisation (L1) and MSE: 0.317686\t\n",
    "# With Regularisation (L2=) and MSE: 0.312463\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa680622-938e-4dd7-98bb-13a3af14a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rue_decoder_best_hp = joblib.load(os.path.join(fp_tuning, str(seed), \"all_rue_decoder_best_hp.joblib\"))\n",
    "all_rue_decoder_best_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a624ea-68ae-492c-8f12-84511b4a553c",
   "metadata": {},
   "source": [
    "#### Train Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177066c-277f-4cb1-a150-b6fc8f004fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rue_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "for time_label, time_info_dict in tqdm(rue_dict.items(), total=len(rue_dict)):\n",
    "    prev_model = load_model(name=f\"rue_predictor_{time_label}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "    hp_dict = all_rue_decoder_best_hp[time_label]\n",
    "    ae_regressor = model_training_decoder(\n",
    "        hp_dict, predictors=predictors, pred_cols=time_info_dict[\"outputs\"], \n",
    "        train_df=time_info_dict[\"train_df\"], valid_df = time_info_dict[\"valid_df\"], seed=seed, prev_model=prev_model,\n",
    "        batch_size=batch_size, max_epochs=10000, verbose=1, patience=20\n",
    "    ) \n",
    "    save_model(model=ae_regressor, name=f\"rue_{time_label}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30107354-1371-4695-b007-9e41add59418",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a891c7-78c0-4f92-a482-b651c0783526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rue_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "for time_label, time_info_dict in tqdm(rue_dict.items(), total=len(rue_dict)):\n",
    "    ae_regressor = load_model(name=f\"rue_{time_label}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "    rue_valid_df = model_test_predictions(\n",
    "        ae_regressor, df_train=time_info_dict[\"train_df\"], df_test=time_info_dict[\"valid_df\"], \n",
    "        pred_cols=time_info_dict[\"outputs\"], predictors=predictors, regressor_label=\"_\"+time_label, pred_min=int(time_label[-1]), T=10, seed=seed)\n",
    "    rue_test_df = model_test_predictions(\n",
    "        ae_regressor, df_train=time_info_dict[\"train_df\"], df_test=time_info_dict[\"test_df\"], \n",
    "        pred_cols=time_info_dict[\"outputs\"], predictors=predictors, regressor_label=\"_\"+time_label, pred_min=int(time_label[-1]), T=10, seed=seed)\n",
    "    display(rue_test_df)\n",
    "    rue_valid_df.to_csv(os.path.join(fp_project_predictions, str(seed), f\"rue_valid_{time_label[-1]}.csv\"))\n",
    "    rue_test_df.to_csv(os.path.join(fp_project_predictions, str(seed), f\"rue_test_{time_label[-1]}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c40e5-c632-44ba-b5a4-c611a1f6c664",
   "metadata": {},
   "source": [
    "## GPR Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9614d05-b23f-4de5-ac8e-0526ecdcef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_gpr(\n",
    "    predictors, pred_cols, train_df, valid_df, seed):\n",
    "\n",
    "    timer = Timer(seed)\n",
    "    \n",
    "    # Get data\n",
    "    train_X, train_y = (\n",
    "        train_df[predictors].values.astype('float64'), train_df[pred_cols].values.astype('float64'))\n",
    "    valid_X, valid_y = (\n",
    "        valid_df[predictors].values.astype('float64'), valid_df[pred_cols].values.astype('float64'))\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_inducing = round(len(train_X)*0.001) # 0.1% of points as inducing points\n",
    "    print(\"- Number of Inducing Points:\", n_inducing)\n",
    "    inducing_points = rng.choice(train_X, size=n_inducing, replace=False)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    def step_callback(step, variables, values):\n",
    "        print(f\"Step {step} {time.time()-start:.3f} s\", end=\"\\r\")\n",
    "\n",
    "    gpr = gpflow.models.SGPR(\n",
    "        (train_X, train_y),\n",
    "        kernel=gpflow.kernels.SquaredExponential(),\n",
    "        inducing_variable=inducing_points,\n",
    "    )\n",
    "    opt = gpflow.optimizers.Scipy()\n",
    "    print(f\"Training started...\")\n",
    "    timer.start(description=\"Training GPR\")\n",
    "    opt.minimize(gpr.training_loss, gpr.trainable_variables, step_callback=step_callback)\n",
    "    timer.end()\n",
    "    # Train GPR\n",
    "    # from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    # from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "    # kernel = DotProduct() + WhiteKernel()\n",
    "    # gpr = GaussianProcessRegressor(kernel=kernel, copy_X_train=False, random_state=seed)\n",
    "    # print(f\"Training started...\")\n",
    "    # start = time.time()\n",
    "    # gpr.fit(train_X, train_y)\n",
    "    \n",
    "    \n",
    "    return gpr\n",
    "\n",
    "def model_test_predictions_gpr(\n",
    "    gpr, df_test, pred_cols, predictors, regressor_label, pred_min, seed=seed):\n",
    "    timer = Timer(seed)\n",
    "    df_test = df_test.copy()\n",
    "  \n",
    "    test_X, test_y = (\n",
    "        df_test[predictors].values.astype('float64'), df_test[pred_cols].values.astype('float64'))\n",
    "    timer.start(description=\"Predicting with GPR\")\n",
    "    test_y_pred, test_std = gpr.compiled_predict_y(test_X)\n",
    "    timer.end()\n",
    "\n",
    "    # test_y_pred, test_std = gpr.predict(test_X, return_std=True)\n",
    "#     print(test_std.shape)\n",
    "    predicted_colnames = [col+\"_gpr\"+regressor_label for col in pred_cols]\n",
    "    std_colnames = [col+\"_gpr_std\"+regressor_label for col in pred_cols]\n",
    "    gpr_mean_std_colname = \"gpr_std_mean\"\n",
    "    df_test[predicted_colnames] = test_y_pred\n",
    "    df_test[std_colnames] = test_std\n",
    "    df_test[gpr_mean_std_colname] = np.mean(test_std, axis=1)\n",
    "    \n",
    "    if df_test['target_index'].dtype != \"int64\":\n",
    "        df_test['target_index'] = df_test['target_index'].apply(lambda x: eval(x)[pred_min-1])\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "def save_model_gpr(model, name, fp_checkpoints):\n",
    "    model.compiled_predict_y = tf.function(\n",
    "        lambda Xnew: model.predict_y(Xnew),\n",
    "        input_signature=[tf.TensorSpec(shape=[None, len(predictors)], dtype=tf.float64)],\n",
    "    )\n",
    "    tf.saved_model.save(model, os.path.join(fp_checkpoints, name))\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "def load_model_gpr(name, fp_checkpoints): \n",
    "    import pickle\n",
    "    model_folder = os.path.join(fp_checkpoints, name)\n",
    "    \n",
    "    if not os.path.exists(model_folder):\n",
    "        print(\"model checkpoint does not exist!\")\n",
    "        return\n",
    "    \n",
    "    return tf.saved_model.load(model_folder)\n",
    "    \n",
    "def calculate_mse(df_test, pred_cols, model_label, regressor_label):\n",
    "    y_true = df_test[pred_cols].values\n",
    "    predicted_cols = [col+model_label+regressor_label for col in pred_cols]\n",
    "    y_pred = df_test[predicted_cols].values\n",
    "    return sklearn.metrics.mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da991c1a-93d0-400f-8582-b3a63d89a033",
   "metadata": {},
   "source": [
    "### Predict T+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badef647-ccbf-4f24-b365-e0da39f7cd1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpr_1 = model_training_gpr( # 88.14491534233093s.\n",
    "    predictors=predictors, pred_cols=pred_cols_1, \n",
    "    train_df=df_train_1, valid_df = df_valid_1, seed=seed\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71524fd-bdd3-47a8-9127-9ca02a89b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_gpr(model=gpr_1, name=\"gpr_1\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df3890-ff03-44d5-ae77-90138b46d3fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpr_1 = load_model_gpr(name=\"gpr_1\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b28d8-2e2a-4ba2-8799-2e05e0e9068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_1_valid_pred = model_test_predictions_gpr(\n",
    "    gpr=gpr_1, df_test=df_valid_1, pred_cols=pred_cols_1, \n",
    "    predictors=predictors, regressor_label=\"_t+1\", pred_min=1)\n",
    "gpr_1_valid_pred.to_csv(os.path.join(fp_project_predictions, str(seed), \"gpr_valid_1.csv\"))\n",
    "gpr_1_test_pred = model_test_predictions_gpr(\n",
    "    gpr=gpr_1, df_test=df_test_1, pred_cols=pred_cols_1, \n",
    "    predictors=predictors, regressor_label=\"_t+1\", pred_min=1)\n",
    "gpr_1_test_pred.to_csv(os.path.join(fp_project_predictions, str(seed), \"gpr_test_1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02405b7-f376-43b2-b3fe-23f2b01e5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mse(df_test=gpr_1_test_pred, pred_cols=pred_cols_1, model_label=\"_gpr\", regressor_label=\"_t+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a62e24-4145-47f6-88c8-52f1d9cbc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpr_1 = load_model_gpr(name=\"gpr_1\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0b9e3-58f3-46a9-b113-d374f17a8d69",
   "metadata": {},
   "source": [
    "### Predict T+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58d5b9-2a9f-4801-b103-e9674e1cd067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpr_2 = model_training_gpr( \n",
    "    predictors=predictors, pred_cols=pred_cols_2, \n",
    "    train_df=df_train_2, valid_df = df_valid_2, seed=seed\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2360626-9992-4d02-9f70-2a146c9d5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_gpr(model=gpr_2, name=\"gpr_2\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e73c8e-4105-4b3a-84e1-44aad2c53b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_2 = load_model_gpr(name=\"gpr_2\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da748b-57f6-4e04-aa2f-df31e796a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_2_valid_pred = model_test_predictions_gpr(\n",
    "    gpr=gpr_2, df_test=df_valid_2, pred_cols=pred_cols_2, \n",
    "    predictors=predictors, regressor_label=\"_t+2\", pred_min=2)\n",
    "gpr_2_valid_pred.to_csv(os.path.join(fp_project_predictions, str(seed), \"gpr_valid_2.csv\"))\n",
    "gpr_2_test_pred = model_test_predictions_gpr(\n",
    "    gpr=gpr_2, df_test=df_test_2, pred_cols=pred_cols_2, \n",
    "    predictors=predictors, regressor_label=\"_t+2\", pred_min=2)\n",
    "gpr_2_test_pred.to_csv(os.path.join(fp_project_predictions, str(seed), \"gpr_test_2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4fca4-15de-41be-ad72-3cfe7f66f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mse(df_test=gpr_2_test_pred, pred_cols=pred_cols_2, model_label=\"_gpr\", regressor_label=\"_t+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9755f0d-fdb8-4ab3-b169-79e55615c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpr_2 = load_model_gpr(name=\"gpr_2\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f2e10-eed1-4203-8454-0927963e1371",
   "metadata": {},
   "source": [
    "### Predict T+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375cd38-c8f8-4f1b-b43a-0d15ec52b990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpr_3 = model_training_gpr( \n",
    "    predictors=predictors, pred_cols=pred_cols_3, \n",
    "    train_df=df_train_3, valid_df = df_valid_3, seed=seed\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b498c-468a-40d7-bc63-23bde0056a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_gpr(model=gpr_3, name=\"gpr_3\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25076fa-9fbd-43ef-9261-03556c9cd7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_3 = load_model_gpr(name=\"gpr_3\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c21de-5f59-4bba-ba3c-475713936819",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_3_valid_pred = model_test_predictions_gpr(\n",
    "    gpr=gpr_3, df_test=df_valid_3, pred_cols=pred_cols_3, \n",
    "    predictors=predictors, regressor_label=\"_t+3\", pred_min=3)\n",
    "gpr_3_valid_pred.to_csv(os.path.join(fp_project_predictions, str(seed), \"gpr_valid_3.csv\"))\n",
    "gpr_3_test_pred = model_test_predictions_gpr(\n",
    "    gpr=gpr_3, df_test=df_test_3, pred_cols=pred_cols_3, \n",
    "    predictors=predictors, regressor_label=\"_t+3\", pred_min=3)\n",
    "gpr_3_test_pred.to_csv(os.path.join(fp_project_predictions, str(seed), \"gpr_test_3.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c994c8-d1f9-4dc1-920c-09e8653aa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mse(df_test=gpr_3_test_pred, pred_cols=pred_cols_3, model_label=\"_gpr\", regressor_label=\"_t+3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeddc279-3153-4094-9fbe-5859422a9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpr_3 = load_model_gpr(name=\"gpr_3\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a31f92-f533-45fa-afb8-423c1d16f82a",
   "metadata": {},
   "source": [
    "## InferNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1105de0-f51c-466c-aa1c-ea61f84ec548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_fn(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true-y_pred), axis=-1)\n",
    "\n",
    "def tune_infernoise(ae_predictor, stddev_list, valid_df, inputs, outputs, seed, T, regressor_label):\n",
    "    corr_list = []\n",
    "    loss_list = []\n",
    "    for stddev in tqdm(stddev_list):\n",
    "        valid_pred_df = infernoise_test_predictions(ae_predictor, valid_df, inputs, outputs, seed, T, stddev, regressor_label, False)\n",
    "        loss = valid_pred_df[\"infernoise_mae\"]\n",
    "        ue = valid_pred_df[\"infernoise_uncertainty\"]\n",
    "        corr, _ = pearsonr(ue, loss)\n",
    "        loss_list.append(valid_pred_df[\"infernoise_mae\"].mean())\n",
    "        corr_list.append(corr)\n",
    "    tuning_df = pd.DataFrame({\"std\": stddev_list, \"correlation\": corr_list, \"loss\":loss_list})\n",
    "    tuning_df[\"best_hyperparameter\"] = False\n",
    "    best_index = tuning_df[\"loss\"].idxmin()\n",
    "    tuning_df.iloc[best_index, -1] = True\n",
    "    return tuning_df\n",
    "\n",
    "def infernoise_test_predictions(ae_predictor, test_df, inputs, outputs, seed, T, stddev, regressor_label, time_run=True):\n",
    "    timer = Timer(seed=seed)\n",
    "    test_df = test_df.copy()\n",
    "    # Process data\n",
    "    test_X, test_y = (test_df[inputs].values.astype('float32'), test_df[outputs].values.astype('float32'))\n",
    "    \n",
    "    set_seed(seed)\n",
    "\n",
    "    # Define model\n",
    "    input_layer = tf.keras.Input(shape=(len(inputs),))\n",
    "    gaussian_noise_layer = tf.keras.layers.GaussianNoise(stddev, seed=seed)\n",
    "    x = ae_predictor.encoder(input_layer)\n",
    "    x = gaussian_noise_layer(x)\n",
    "    x = ae_predictor.regressor.layers[-1](x) # To ignore dropout layer\n",
    "    infernoise_model = tf.keras.Model(inputs=input_layer, outputs=x, name=\"infernoise_model\")\n",
    "    \n",
    "    # For Infer Noise\n",
    "    # - Sample with infer noise\n",
    "    test_y_sample_preds = [] # T, num samples, output classes\n",
    "    if time_run:\n",
    "        timer.start(description=\"Predicting with Infer-Noise\")\n",
    "    for i in range(T):\n",
    "        test_y_pred = infernoise_model(test_X, training=True)\n",
    "        # print(test_y_pred[:5])\n",
    "        test_y_sample_preds.append(test_y_pred)\n",
    "    if time_run:\n",
    "        timer.end()\n",
    "    \n",
    "    test_y_sample_preds = np.array(test_y_sample_preds)\n",
    "    test_y_sample_preds =  test_y_sample_preds.transpose((1, 2, 0)) # num samples, predicted features, T\n",
    "    \n",
    "    # - Get mean prediction \n",
    "    # num samples, predicted features\n",
    "    test_y_mean_pred = np.mean(test_y_sample_preds, axis=-1)\n",
    "    predicted_infernoise_colnames = [col + \"_infernoise\"+regressor_label for col in outputs]\n",
    "    test_df[predicted_infernoise_colnames] = test_y_mean_pred\n",
    "    \n",
    "    # - Get loss for each output\n",
    "    test_y_mc_loss = mae_fn(y_true=test_y, y_pred=test_y_mean_pred)\n",
    "    test_df[\"infernoise_mae\"] = test_y_mc_loss\n",
    "    \n",
    "    # - Uncertainty score (for regression)\n",
    "    #   - Calculate std of predictions\n",
    "    test_y_sd_pred = np.mean(np.std(test_y_sample_preds, axis=-1, ddof=1), axis=-1)\n",
    "    test_df[\"infernoise_uncertainty\"] = test_y_sd_pred\n",
    "\n",
    "    return test_df\n",
    "\n",
    "def display_tuning_df(tuning_df):\n",
    "    def highlight_sentiment(row):\n",
    "        if row[\"best_hyperparameter\"]:\n",
    "            return ['background-color: #cafae6'] * len(row)\n",
    "        else:\n",
    "            return [''] * len(row)\n",
    "    return tuning_df.style.apply(highlight_sentiment, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724fb4b-3ebe-44cc-939e-f15fb83670e2",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e33a06-32a8-4dff-990c-a8cca5c667ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infernoise_dict = {\n",
    "    \"t+1\": {\"model_name\": \"rue_t+1\", \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"model_name\": \"rue_t+2\", \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"model_name\": \"rue_t+3\", \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "infernoise_hp_dict = {}\n",
    "for time_label, info_dict in infernoise_dict.items():\n",
    "    print(f\"{time_label}:\")\n",
    "    ae_regressor = load_model(name=info_dict[\"model_name\"], fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "    infernoise_tuning_df = tune_infernoise(\n",
    "        ae_regressor, stddev_list=[0.00001, 0.00005, 0.0001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5], \n",
    "        valid_df=info_dict[\"valid_df\"], inputs=predictors, outputs=info_dict[\"outputs\"], seed=seed, T=10, regressor_label=\"_\"+time_label\n",
    "    )\n",
    "    display(display_tuning_df(infernoise_tuning_df))\n",
    "    infernoise_hp_dict[time_label] = infernoise_tuning_df.iloc[infernoise_tuning_df[\"loss\"].argmin(), 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3e403-f933-4e65-ad5d-ede6345319da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(infernoise_hp_dict, os.path.join(fp_tuning, \"all_infernoise_best_hp.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7e8ff-90a5-498b-bcdc-5c11a42a3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "infernoise_hp_dict = joblib.load(os.path.join(fp_tuning, \"all_infernoise_best_hp.joblib\"))\n",
    "infernoise_hp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43216fe-dc07-43ea-91e7-1202217318d6",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d90d53-9a8f-4d65-a2e3-971058aa5662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for time_label, info_dict in infernoise_dict.items():\n",
    "    print(f\"{time_label}:\")\n",
    "    ae_regressor = load_model(name=info_dict[\"model_name\"], fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "    infernoise_valid_df = infernoise_test_predictions(\n",
    "         ae_regressor, test_df=info_dict[\"valid_df\"], inputs=predictors, outputs=info_dict[\"outputs\"], regressor_label=\"_\"+time_label, \n",
    "        seed=seed, T=10, stddev=infernoise_hp_dict[time_label])\n",
    "    infernoise_test_df = infernoise_test_predictions(\n",
    "         ae_regressor, test_df=info_dict[\"test_df\"], inputs=predictors, outputs=info_dict[\"outputs\"], regressor_label=\"_\"+time_label, \n",
    "        seed=seed, T=10, stddev=infernoise_hp_dict[time_label])\n",
    "    display(infernoise_test_df)\n",
    "    infernoise_valid_df.to_csv(os.path.join(fp_project_predictions, str(seed), f\"infernoise_valid_{time_label[-1]}.csv\"))\n",
    "    infernoise_test_df.to_csv(os.path.join(fp_project_predictions, str(seed), f\"infernoise_test_{time_label[-1]}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ecaf07-cc83-4913-8cd8-36cc6d8c8a19",
   "metadata": {},
   "source": [
    "## BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6febf-3b31-491f-baf9-ecc1304837b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def mae_fn(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true-y_pred), axis=-1)\n",
    "\n",
    "def set_seed_pytorch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, feat_cols, target_cols):\n",
    "        self.data_df = df\n",
    "        self.feat_cols = feat_cols\n",
    "        self.target_cols = target_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_df.iloc[idx]\n",
    "        features = torch.from_numpy(row[self.feat_cols].values.astype(float)).float()\n",
    "        label = torch.from_numpy(row[self.target_cols].values.astype(float)).float()\n",
    "        return features, label\n",
    "\n",
    "def instantiate_bnn_model(num_layers, width, num_inputs, num_outputs, seed):\n",
    "    set_seed_pytorch(seed)\n",
    "    layers = []\n",
    "    in_features, out_features = num_inputs, width\n",
    "    for i in range(num_layers):\n",
    "        if (i == (num_layers-1)):\n",
    "            out_features = num_outputs\n",
    "        layers.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=in_features, out_features=out_features))\n",
    "        if (i != num_layers-1):\n",
    "            layers.append(nn.ReLU())\n",
    "        in_features = width\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def train_bnn_model(bnn_model, train_df, valid_df, feat_cols, target_cols, epochs, patience, seed, fp_model):\n",
    "    set_seed_pytorch(seed)\n",
    "    # Prepare dataset\n",
    "    train_ds = TabularDataset(df=train_df, feat_cols=feat_cols, target_cols=target_cols)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Prepare dataset\n",
    "    set_seed_pytorch(seed)\n",
    "    valid_ds = TabularDataset(df=valid_df, feat_cols=feat_cols, target_cols=target_cols)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    mse_loss = nn.MSELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "    kl_weight = 0.1\n",
    "    optimizer = optim.Adam(bnn_model.parameters(), lr=0.001)\n",
    "\n",
    "    best_epoch, best_val_loss, patience_count = -1, np.inf, 0\n",
    "\n",
    "    bnn_model.to(device)\n",
    "\n",
    "    with tqdm(range(epochs), total=epochs) as pbar:\n",
    "        for epoch in pbar:\n",
    "            for x_batch, y_batch in train_dl:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                pred = bnn_model(x_batch)\n",
    "                mse = mse_loss(pred, y_batch)\n",
    "                kl = kl_loss(bnn_model)\n",
    "                cost = mse + kl_weight*kl\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                cost.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                x_batch = x_batch.detach().cpu()\n",
    "                y_batch = y_batch.detach().cpu()\n",
    "                mse = mse.detach().cpu()\n",
    "                kl = kl.detach().cpu()\n",
    "                cost = cost.detach().cpu()\n",
    "                \n",
    "            # Evaluate performance on validation set\n",
    "            valid_pred = predict_bnn_model(bnn_model, valid_dl, feat_cols, target_cols, seed=seed, silent=True)\n",
    "            valid_loss = evaluate_bnn_perf(valid_df, feat_cols, target_cols, valid_pred)\n",
    "            pbar.set_description(f\"valid_loss: {valid_loss:.5f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if valid_loss < best_val_loss:\n",
    "                best_epoch, best_val_loss = epoch, valid_loss\n",
    "                patience_count = 0\n",
    "                torch.save(bnn_model, fp_model)\n",
    "            else:\n",
    "                patience_count += 1\n",
    "                if patience_count > patience:\n",
    "                    print(f\"Early stopping! Model achieved best performance at Epoch {best_epoch} with loss = {best_val_loss}.\")\n",
    "                    break\n",
    "    \n",
    "    return best_val_loss, best_epoch\n",
    "        \n",
    "def predict_bnn_model(bnn_model, dl, feat_cols, target_cols, seed, silent):\n",
    "    set_seed_pytorch(seed)\n",
    "    # Send model to gpu\n",
    "    bnn_model.to(device)\n",
    "    # Predictions\n",
    "    all_pred = []\n",
    "    # Predict with model\n",
    "    with tqdm(dl, total=len(dl), disable=silent) as pbar:\n",
    "        for x_batch, y_batch in pbar:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            pred = bnn_model(x_batch)\n",
    "            all_pred.append(pred.detach().cpu())\n",
    "    return torch.cat(all_pred)\n",
    "\n",
    "def evaluate_bnn_perf(df, feat_cols, target_cols, pred):\n",
    "    y = df[target_cols].values\n",
    "    mse = torch.mean(torch.square(pred-y)).item()\n",
    "    return mse\n",
    "\n",
    "def evaluate_bnn(bnn_model, df, feat_cols, target_cols, seed):\n",
    "    ds = TabularDataset(df=df, feat_cols=feat_cols, target_cols=target_cols)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    pred = predict_bnn_model(bnn_model, dl, feat_cols, target_cols, seed=seed, silent=True)\n",
    "    return evaluate_bnn_perf(df, feat_cols, target_cols, pred)\n",
    "\n",
    "def tune_bnn_model(param_grid, train_df, valid_df, feat_cols, target_cols, epochs, patience, seed, fp_model):\n",
    "    parameter_list = list(ParameterGrid(param_grid))\n",
    "    loss_list, time_list, epoch_list = [], [], []\n",
    "    with tqdm(parameter_list) as pbar:\n",
    "        for param_dict in pbar:\n",
    "            bnn_model = instantiate_bnn_model(seed=seed, num_inputs=len(feat_cols), num_outputs=len(target_cols), **param_dict)\n",
    "            start = time.time()\n",
    "            loss, best_epoch = train_bnn_model(bnn_model, train_df, valid_df, feat_cols, target_cols, epochs, patience, seed, fp_model)\n",
    "            time_list.append(time.time()-start)\n",
    "            epoch_list.append(best_epoch)\n",
    "            loss_list.append(loss)\n",
    "    tuning_df = pd.DataFrame(parameter_list)\n",
    "    tuning_df[\"loss\"] = loss_list\n",
    "    tuning_df[\"epoch\"] = epoch_list\n",
    "    tuning_df[\"time/s\"] = time_list\n",
    "    best_index = np.argmin(tuning_df[\"loss\"])\n",
    "    tuning_df[\"best_hyperparameter\"] = False\n",
    "    tuning_df.iloc[best_index, -1] = True\n",
    "    return tuning_df, parameter_list[best_index]\n",
    "\n",
    "def train_model_w_best_param(best_param, train_df, valid_df, feat_cols, target_cols, epochs, patience, seed, fp_model):\n",
    "    bnn_model =  instantiate_bnn_model(seed=seed, num_inputs=len(feat_cols), num_outputs=len(target_cols), **best_param)\n",
    "    timer = Timer(seed=seed)\n",
    "    timer.start(description=\"Training BNN\")\n",
    "    train_bnn_model(bnn_model, train_df, valid_df, feat_cols, target_cols, epochs, patience, seed, fp_model)\n",
    "    timer.end()\n",
    "    return torch.load(fp_model)\n",
    "\n",
    "def bnn_model_prediction(bnn_model, test_df, feat_cols, target_cols, T, seed,  regressor_label):\n",
    "    test_df = test_df.copy()\n",
    "    set_seed_pytorch(seed)\n",
    "    # Prepare dataset\n",
    "    test_ds = TabularDataset(df=test_df, feat_cols=feat_cols, target_cols=target_cols)\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    seed_list = list(range(seed, seed+T))\n",
    "    all_logits = []\n",
    "    timer = Timer(seed=seed)\n",
    "    timer.start(description=\"Predicting with BNN\")\n",
    "    for cur_seed in tqdm(seed_list):\n",
    "        logits = predict_bnn_model(bnn_model, test_dl, feat_cols, target_cols, seed=cur_seed, silent=True)\n",
    "        all_logits.append(logits)\n",
    "    timer.end()\n",
    "    \n",
    "    all_logits = torch.stack(all_logits) # T, N, O\n",
    "    test_y_pred = all_logits.mean(axis=0) # N, O\n",
    "    test_y_std = all_logits.std(axis=0) # N, O\n",
    "    test_y = torch.from_numpy(test_df[target_cols].values).float()\n",
    "    \n",
    "    test_df[\"bnn_uncertainty\"] = test_y_std.mean(axis=-1) # N\n",
    "    test_df[\"bnn_mae\"] = mae_fn(y_true=test_y.numpy(), y_pred=test_y_pred.numpy())\n",
    "    predicted_colnames = [col + \"_bnn_\"+ regressor_label for col in target_cols]\n",
    "    test_df[predicted_colnames] = test_y_pred\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ffec0-ffc6-49d7-bcdd-1d4cd0865182",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9649c944-ede9-4f8d-ae06-b256b2543cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_width_1=256\n",
    "ae_regressor_1 = model_training(\n",
    "    width=best_width_1, predictors=predictors, pred_cols=pred_cols_1, \n",
    "    train_df=df_train_1, valid_df = df_valid_1, seed=seed,\n",
    "    batch_size=2048, max_epochs=10000, verbose=1, patience=20\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916607c-0165-4708-a7b7-65152a5a4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "all_bnn_best_hp = {}\n",
    "for time_label, time_info_dict in bnn_dict.items():\n",
    "    fp_model = os.path.join(fp_project_models, str(seed), f\"bnn_{time_label}.pt\")\n",
    "    bnn_tuning_df, bnn_best_hp = tune_bnn_model(\n",
    "        param_grid={\"num_layers\":[2, 3], \"width\":[32, 64, 128, 256]}, \n",
    "        train_df=time_info_dict[\"train_df\"], valid_df=time_info_dict[\"valid_df\"], \n",
    "        feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], epochs=500, patience=5, seed=seed, fp_model=fp_model)\n",
    "    display(bnn_tuning_df)\n",
    "    bnn_tuning_df.to_csv(os.path.join(fp_tuning, str(seed), f\"tuning_bnn_{time_label}.csv\"))\n",
    "    all_bnn_best_hp[time_label] = bnn_best_hp\n",
    "joblib.dump(all_bnn_best_hp, os.path.join(fp_tuning, \"all_bnn_best_hp.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7075e27-aae4-4d27-86c9-133644f57d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bnn_best_hp = joblib.load(os.path.join(fp_tuning, str(seed), \"all_bnn_best_hp.joblib\"))\n",
    "print(all_bnn_best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de1306-14ad-4d94-8a76-1434b4f7b610",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40ae30-74a2-46a2-af98-34681bfdbdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "for time_label, time_info_dict in tqdm(bnn_dict.items(), total=len(bnn_dict)):\n",
    "    fp_model = os.path.join(fp_project_models, str(seed), f\"bnn_{time_label}.pt\")\n",
    "    bnn_best_hp = all_bnn_best_hp[time_label]\n",
    "    bnn_model = train_model_w_best_param(\n",
    "        best_param=bnn_best_hp, train_df=time_info_dict[\"train_df\"], valid_df=time_info_dict[\"valid_df\"], \n",
    "        feat_cols=predictors, target_cols=time_info_dict[\"outputs\"],\n",
    "        epochs=500, patience=5, seed=seed, fp_model=fp_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d12e4-8b1e-4206-98fa-daee934b3be2",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b505f6e-1c16-4ba3-9dd1-8607328f4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "for time_label, time_info_dict in tqdm(bnn_dict.items(), total=len(bnn_dict)):\n",
    "    fp_model = os.path.join(fp_project_models, str(seed), f\"bnn_{time_label}.pt\")\n",
    "    bnn_model = torch.load(fp_model)\n",
    "    bnn_valid_df = bnn_model_prediction(bnn_model, time_info_dict[\"valid_df\"], feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], T=10, seed=seed, regressor_label=time_label)\n",
    "    bnn_test_df = bnn_model_prediction(bnn_model, time_info_dict[\"test_df\"], feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], T=10, seed=seed, regressor_label=time_label)\n",
    "    display(bnn_test_df)\n",
    "    bnn_valid_df.to_csv(os.path.join(fp_project_predictions, str(seed), f\"bnn_valid_{time_label[-1]}.csv\"))\n",
    "    bnn_test_df.to_csv(os.path.join(fp_project_predictions, str(seed), f\"bnn_test_{time_label[-1]}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455ad27-cc84-4a37-8ff8-12a26e8534b6",
   "metadata": {},
   "source": [
    "## DER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440c347-92b8-4052-a8c8-e4029ce81e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "enable_print  = print\n",
    "disable_print = lambda *x, **y: None\n",
    "\n",
    "def mae_fn(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true-y_pred), axis=-1)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, feat_cols, target_cols):\n",
    "        self.data_df = df\n",
    "        self.feat_cols = feat_cols\n",
    "        self.target_cols = target_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_df.iloc[idx]\n",
    "        features = torch.from_numpy(row[self.feat_cols].values.astype(float)).float()\n",
    "        label = torch.from_numpy(row[self.target_cols].values.astype(float)).float()\n",
    "        return features, label\n",
    "\n",
    "def optim_regression(\n",
    "    model: nn.Module, learning_rate: float = 0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0,)\n",
    "    return optimizer\n",
    "\n",
    "def train_der_w_param(n_hidden_layers, hidden_width, train_df, valid_df, inputs, outputs, seed, max_epochs=500, patience=5, time_training=True):\n",
    "    seed_everything(seed, workers=True)\n",
    "    timer = Timer(seed=seed)\n",
    "    # Data\n",
    "    train_ds = TabularDataset(df=train_df, feat_cols=inputs, target_cols=outputs)\n",
    "    valid_ds = TabularDataset(df=valid_df, feat_cols=inputs, target_cols=outputs)\n",
    "    datamodule = LightningDataModule.from_datasets(train_ds, val_dataset=valid_ds, test_dataset=valid_ds, batch_size=batch_size, num_workers=63)\n",
    "    datamodule.training_task = \"regression\"\n",
    "    # print(\"hidden_dims:\", [hidden_width for _ in range(n_hidden_layers)])\n",
    "    # Model\n",
    "    model = mlp(\n",
    "        in_features=len(inputs),\n",
    "        num_outputs=4*len(outputs),\n",
    "        hidden_dims=[hidden_width for _ in range(n_hidden_layers)],\n",
    "        final_layer=NormalInverseGammaLayer,\n",
    "        final_layer_args={\"dim\": len(outputs)},\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    loss = DERLoss(reg_weight=1e-2)\n",
    "    routine = RegressionRoutine(\n",
    "        probabilistic=True,\n",
    "        output_dim=len(outputs),\n",
    "        model=model,\n",
    "        loss=loss,\n",
    "        optim_recipe=optim_regression(model),\n",
    "    )\n",
    "    early_stopping = EarlyStopping('val/MSE', patience=patience, verbose=True, mode='min')\n",
    "\n",
    "    trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=max_epochs, enable_progress_bar=True, callbacks=[early_stopping])\n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        if time_training:\n",
    "            timer.start(description=\"Training DER\")\n",
    "        trainer.fit(model=routine, datamodule=datamodule)\n",
    "        if time_training:\n",
    "            timer.end()\n",
    "        result = trainer.test(model=routine, datamodule=datamodule)\n",
    "\n",
    "    return model, result[0]\n",
    "\n",
    "def der_model_prediction(der_model, test_df, feat_cols, target_cols, seed, silent, regressor_label):\n",
    "    seed_everything(seed, workers=True)\n",
    "    test_df = test_df.copy()\n",
    "\n",
    "    timer = Timer(seed=seed)\n",
    "    timer.start(description=\"Predicting with DER\")\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(test_df[feat_cols].values).float()\n",
    "        dists = der_model(x)\n",
    "        means = dists.loc\n",
    "        std = torch.sqrt(dists.variance_loc)\n",
    "    timer.end()\n",
    "    \n",
    "    test_y = torch.from_numpy(test_df[target_cols].values).float()\n",
    "    \n",
    "    test_df[\"der_uncertainty\"] = std.mean(axis=-1) # N\n",
    "    test_df[\"der_mae\"] = mae_fn(y_true=test_y.numpy(), y_pred=means.numpy())\n",
    "    predicted_colnames = [col + \"_der_\"+regressor_label for col in target_cols]\n",
    "    test_df[predicted_colnames] = means\n",
    "    return test_df\n",
    "\n",
    "def tune_der_model(param_grid, train_df, valid_df, feat_cols, target_cols, epochs, patience, seed,):\n",
    "    parameter_list = list(ParameterGrid(param_grid))\n",
    "    loss_list, time_list = [], []\n",
    "    with tqdm(parameter_list) as pbar:\n",
    "        for param_dict in pbar:\n",
    "            start = time.time()\n",
    "            der_model, result = train_der_w_param(\n",
    "                train_df=train_df, valid_df=valid_df, \n",
    "                inputs=feat_cols, outputs=target_cols,\n",
    "                seed=seed, max_epochs=epochs, patience=patience, **param_dict\n",
    "            )\n",
    "            # print(result)\n",
    "            time_list.append(time.time()-start)\n",
    "            loss_list.append(result['test/MSE'])\n",
    "    tuning_df = pd.DataFrame(parameter_list)\n",
    "    tuning_df[\"loss\"] = loss_list\n",
    "    tuning_df[\"time/s\"] = time_list\n",
    "    best_index = np.argmin(tuning_df[\"loss\"])\n",
    "    tuning_df[\"best_hyperparameter\"] = False\n",
    "    tuning_df.iloc[best_index, -1] = True\n",
    "    return tuning_df, parameter_list[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd670b-a269-45ef-a4d3-21950e6ba98a",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ba557-705e-415a-a119-394bf21e6cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "der_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "all_der_best_hp = {}\n",
    "for time_label, time_info_dict in tqdm(der_dict.items(), total=len(der_dict)):\n",
    "    der_tuning_df, der_best_hp = tune_der_model(\n",
    "        param_grid={\n",
    "            \"n_hidden_layers\":[2, 3, 4],\n",
    "            \"hidden_width\": [128, 256, 512]}, \n",
    "        train_df=time_info_dict[\"train_df\"], valid_df=time_info_dict[\"valid_df\"], \n",
    "        feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], epochs=500, patience=5, seed=seed\n",
    "    )\n",
    "    der_tuning_df.to_csv(os.path.join(fp_tuning, str(seed), f\"tuning_der_{time_label}.csv\"))\n",
    "    all_der_best_hp[time_label] = der_best_hp\n",
    "    display(der_tuning_df)\n",
    "joblib.dump(all_der_best_hp, os.path.join(fp_tuning, str(seed), \"all_der_best_hp.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2fd2c-7d17-4c78-9039-a3b984a523ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_der_best_hp = joblib.load(os.path.join(fp_tuning, \"all_der_best_hp.joblib\"))\n",
    "all_der_best_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d63a5-7737-4b35-8a85-26c08099a458",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-15T02:35:25.754003Z",
     "iopub.status.idle": "2024-10-15T02:35:25.754156Z",
     "shell.execute_reply": "2024-10-15T02:35:25.754085Z",
     "shell.execute_reply.started": "2024-10-15T02:35:25.754077Z"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650ce5b-76a4-4179-81b3-9ac814aa0e25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "der_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "for time_label, time_info_dict in tqdm(der_dict.items(), total=len(der_dict)):\n",
    "    fp_model = os.path.join(fp_project_models, str(seed), f\"der_{time_label}.pt\")\n",
    "    der_model, _ = train_der_w_param(\n",
    "        **all_der_best_hp[time_label], \n",
    "        train_df=time_info_dict[\"train_df\"], valid_df=time_info_dict[\"valid_df\"], \n",
    "        inputs=predictors, outputs=time_info_dict[\"outputs\"],\n",
    "        seed=seed, max_epochs=500, patience=5\n",
    "    )\n",
    "    torch.save(der_model, fp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc2e41-d9fa-44a6-98d1-9f13fd9d60ad",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ba88b-4a31-48ea-a2e0-891d72900854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "der_dict = {\n",
    "    \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "    \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "    \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "}\n",
    "for time_label, time_info_dict in tqdm(der_dict.items(), total=len(der_dict)):\n",
    "    fp_model = os.path.join(fp_project_models, str(seed), f\"der_{time_label}.pt\")\n",
    "    der_model = torch.load(fp_model)\n",
    "    der_valid_df = der_model_prediction(\n",
    "        der_model, test_df=time_info_dict[\"valid_df\"], feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], seed=seed, silent=False, regressor_label=time_label)\n",
    "    der_test_df = der_model_prediction(\n",
    "        der_model, test_df=time_info_dict[\"test_df\"], feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], seed=seed, silent=False, regressor_label=time_label)\n",
    "    display(der_test_df)\n",
    "    der_valid_df.to_csv(os.path.join(fp_project_predictions, str(seed), f\"der_valid_{time_label[-1]}.csv\"))\n",
    "    der_test_df.to_csv(os.path.join(fp_project_predictions, str(seed), f\"der_test_{time_label[-1]}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827343b9-f052-47af-b810-d2dbc1039bb8",
   "metadata": {},
   "source": [
    "## Generate Prediction Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b52aa-1d0c-4883-9454-1d222ddc2572",
   "metadata": {},
   "source": [
    "#### Load Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0096434-2247-46bf-bf29-cedfa19c504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scaler(fp_downsampled_scaler_file):\n",
    "    import pickle \n",
    "    with open(fp_downsampled_scaler_file, 'rb') as handle:\n",
    "        scaler = pickle.load(handle)\n",
    "    return scaler\n",
    "\n",
    "def load_all_predictions(time_label, split=\"test\"):\n",
    "    columns = []\n",
    "    df_list = [] \n",
    "    for model in [\"rue\", \"gpr\", \"infernoise\", \"bnn\", \"der\"]: # Update this when you have a new ue\n",
    "        fp = os.path.join(fp_project_predictions, str(seed), f\"{model}_{split}_{time_label}.csv\")\n",
    "        df = pd.read_csv(fp, index_col=0)\n",
    "        # find new columns not in current column list\n",
    "        new_cols = [col for col in df.columns if col not in columns]\n",
    "        columns.extend(new_cols)\n",
    "        df_list.append(df[new_cols])\n",
    "    return pd.concat(df_list, axis=1)\n",
    "\n",
    "# Save all predictions\n",
    "def save_df_dict(df_dict, seed):\n",
    "    for time_label, time_info in tqdm(df_dict.items()):\n",
    "        fp_seed_folder = os.path.join(fp_project_pi_predictions, str(seed))\n",
    "        create_folder(fp_seed_folder)\n",
    "        val_df, test_df, pred_cols = time_info[\"valid_df\"], time_info[\"test_df\"], time_info[\"pred_cols\"]\n",
    "        val_df.to_csv(os.path.join(fp_seed_folder, \"val_\"+time_label+\".csv\"))\n",
    "        test_df.to_csv(os.path.join(fp_seed_folder, \"test_\"+time_label+\".csv\"))\n",
    "        joblib.dump(pred_cols, os.path.join(fp_seed_folder, \"pred_cols_\"+time_label+\".joblib\"))\n",
    "    print(\"Saved df_dict!\")\n",
    "\n",
    "# Load all predictions\n",
    "def load_df_dict(seed, time_labels=[\"t+1\", \"t+2\", \"t+3\"]):\n",
    "    df_dict = {}\n",
    "    for time_label in tqdm(time_labels):\n",
    "        fp_seed_folder = os.path.join(fp_project_pi_predictions, str(seed))\n",
    "        val_df = pd.read_csv(os.path.join(fp_seed_folder, \"val_\"+time_label+\".csv\"), index_col=0)\n",
    "        val_df = val_df.loc[:, ~val_df.columns.str.contains('^Unnamed')]\n",
    "        test_df = pd.read_csv(os.path.join(fp_seed_folder, \"test_\"+time_label+\".csv\"), index_col=0)\n",
    "        test_df = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]\n",
    "        pred_cols = joblib.load(os.path.join(fp_seed_folder, \"pred_cols_\"+time_label+\".joblib\"))\n",
    "        df_dict[time_label] = {\"valid_df\": val_df, \"test_df\": test_df, \"pred_cols\":pred_cols}\n",
    "    print(\"Loaded df_dict!\")\n",
    "    return df_dict\n",
    "\n",
    "def add_new_ue_to_df_dict(df_dict, seed, model, time_labels=[\"t+1\", \"t+2\", \"t+3\"]):\n",
    "    for time_label in tqdm(time_labels):\n",
    "        for split in [\"valid_df\", \"test_df\"]:\n",
    "            fp = os.path.join(fp_project_predictions, str(seed), f\"{model}_{split[:-3]}_{time_label[-1]}.csv\")\n",
    "            df = pd.read_csv(fp, index_col=0)\n",
    "            df = df.reset_index(drop=True)\n",
    "            # find new columns not in current column list\n",
    "            new_cols = [col for col in df.columns if col not in df_dict[time_label][split].columns]\n",
    "            if len(new_cols)==0:\n",
    "                print(f\"No new columns in {split}\")\n",
    "                continue\n",
    "            df_dict[time_label][split] = pd.concat([df_dict[time_label][split], df[new_cols]], axis=1)\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e814a8-6930-4325-a192-e77a23b3fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = load_scaler(fp_downsampled_scaler_file)\n",
    "\n",
    "# val_df_1 = load_all_predictions(time_label=1, split=\"valid\")\n",
    "# val_df_2 = load_all_predictions(time_label=2, split=\"valid\")\n",
    "# val_df_3 = load_all_predictions(time_label=3, split=\"valid\")\n",
    "    \n",
    "# test_df_1 = load_all_predictions(time_label=1)\n",
    "# test_df_2 = load_all_predictions(time_label=2)\n",
    "# test_df_3 = load_all_predictions(time_label=3)\n",
    "\n",
    "# df_dict = {\n",
    "#     \"t+1\": {\"valid_df\": val_df_1, \"test_df\": test_df_1, \"pred_cols\":pred_cols_1}, \n",
    "#     \"t+2\": {\"valid_df\": val_df_2, \"test_df\": test_df_2, \"pred_cols\":pred_cols_2}, \n",
    "#     \"t+3\": {\"valid_df\": val_df_3, \"test_df\": test_df_3, \"pred_cols\":pred_cols_3}, \n",
    "# }\n",
    "\n",
    "# df_dict = add_new_ue_to_df_dict(df_dict, seed=seed, model=\"der\", time_labels=[\"t+1\", \"t+2\", \"t+3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2b00d-2af9-4035-9d13-fa3616ab199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = load_df_dict(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b5719-f1d5-421e-a451-a97e4a6a48c5",
   "metadata": {},
   "source": [
    "#### Calculate KNN Prediction Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8847080-5ab5-49ae-a650-92c0fbf346f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_prediction_interval(df_val, df_test, scaler, predictors, pred_cols, pred_label, regressor_label, ue_col, alpha=0.05, seed=seed):\n",
    "    pi_label = \"_knn\"\n",
    "    df_val, df_test = df_val.copy(), df_test.copy()\n",
    "\n",
    "    unscaled_cols = [col+\"_unscaled\" for col in pred_cols]\n",
    "    prediction_cols = [col+pred_label+\"_\"+regressor_label for col in pred_cols]\n",
    "    unscaled_pred_cols = [col+\"_unscaled\" for col in prediction_cols]\n",
    "\n",
    "    # Unscale the prediction columns\n",
    "    df_val[unscaled_cols] = scaler.inverse_transform(df_val[pred_cols])\n",
    "    df_test[unscaled_cols] = scaler.inverse_transform(df_test[pred_cols])\n",
    "    df_val[unscaled_pred_cols] = scaler.inverse_transform(df_val[prediction_cols])\n",
    "    df_test[unscaled_pred_cols] = scaler.inverse_transform(df_test[prediction_cols])\n",
    "\n",
    "    # Set K = sqrt of size of validation set\n",
    "    k = round(np.sqrt(len(df_val)))\n",
    "\n",
    "    # Get reconstruction errors\n",
    "    reconstruction_cols = [col+\"_reconstruction_\"+regressor_label for col in predictors]\n",
    "    valid_re = df_val[predictors].values - df_val[reconstruction_cols].values\n",
    "    test_re = df_test[predictors].values - df_test[reconstruction_cols].values\n",
    "\n",
    "    timer = Timer(seed=seed)\n",
    "    timer.start(description=\"KNN PI\")\n",
    "\n",
    "    # Find neighbours\n",
    "    n_val = len(valid_re)\n",
    "    kdtree = KDTree(valid_re)\n",
    "    dist, ind = kdtree.query(test_re, k=k, workers=-1)\n",
    "\n",
    "    for col in pred_cols:\n",
    "        # 1. Val df\n",
    "        # Get error for each variable\n",
    "        val_y = df_val[col+\"_unscaled\"].values.astype('float32')\n",
    "        val_y_pred = df_val[col+pred_label+\"_\"+regressor_label+\"_unscaled\"].values.astype('float32')\n",
    "        val_pe = np.abs(val_y-val_y_pred)\n",
    "\n",
    "        # Nearbouring prediction errors\n",
    "        neighbour_pe = val_pe[ind]\n",
    "\n",
    "        # PI = 0.95 Percentile of neigbouring errors\n",
    "        df_test[col+\"_\"+ue_col+pi_label] = np.quantile(neighbour_pe, np.ceil((k+1)*(1-alpha))/k, method='higher')\n",
    "        #np.percentile(neighbour_pe, (1-alpha/2)*100, axis=1)\n",
    "\n",
    "    timer.end()\n",
    "    \n",
    "    return df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a4147-7614-4653-83c4-8ae6a51f449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_dict = {\n",
    "    \"RUE\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \n",
    "    },\n",
    "}    \n",
    "\n",
    "# Apply knn_prediction_interval to all RUE\n",
    "for time_label, time_info in df_dict.items():\n",
    "    val_df, test_df, pred_cols = time_info[\"valid_df\"], time_info[\"test_df\"], time_info[\"pred_cols\"]\n",
    "    ue_label = \"rue\"\n",
    "    for ue_label, ue_info in ue_dict.items():\n",
    "        pred_label, ue_col = ue_info[\"pred_label\"], ue_info[\"ue_col\"]\n",
    "        df_dict[time_label][\"test_df\"] = knn_prediction_interval(\n",
    "            df_val=df_dict[time_label][\"valid_df\"], df_test=df_dict[time_label][\"test_df\"], predictors=predictors, pred_cols=pred_cols, \n",
    "            pred_label=pred_label, regressor_label=time_label, ue_col=ue_col, scaler=scaler)\n",
    "\n",
    "save_df_dict(df_dict=df_dict, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3330a-116b-42ba-997d-d73518f1f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in df_dict[\"t+1\"][\"valid_df\"].columns if \"direct\" in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d73b5-d19f-444e-bbef-06b5a2e991d2",
   "metadata": {},
   "source": [
    "#### Calculate Weighted Percentile Prediction Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebb2e9-a6e3-41c4-a811-72d311d77485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_percentile_original(values, weights, percentile): # both are matrices\n",
    "    n_test = len(values)\n",
    "    pis = []\n",
    "    for i in tqdm(range(n_test), total=n_test, leave=False):\n",
    "        df = pd.DataFrame({'v': values[i], 'w': weights[i]})\n",
    "        # display(df)\n",
    "        calc = wc.Calculator('w')  # w designates weight\n",
    "        pi = calc.quantile(df, 'v', percentile)\n",
    "        pis.append(pi)\n",
    "    return pis\n",
    "\n",
    "def weighted_percentile_matrix(values, weights, percentile):\n",
    "    ix = np.argsort(values, axis=-1) # sort within each \n",
    "    values = np.take_along_axis(values, ix, axis=-1) # sort data\n",
    "    weights = np.take_along_axis(weights, ix, axis=-1) # sort weights\n",
    "    cdf = (np.cumsum(weights, axis=-1) - 0.5 * weights) / np.sum(weights, axis=-1, keepdims=True) # 'like' a CDF function\n",
    "    return [np.interp(percentile, cdf[i], values[i]) for i in range(values.shape[0])]\n",
    "\n",
    "def weighted_prediction_interval(df_val, df_test, scaler, predictors, pred_cols, pred_label, regressor_label, ue_col, alpha=0.05, seed=seed):\n",
    "    pi_label = \"_weighted\"\n",
    "    df_val, df_test = df_val.copy(), df_test.copy()\n",
    "\n",
    "    unscaled_cols = [col+\"_unscaled\" for col in pred_cols]\n",
    "    prediction_cols = [col+pred_label+\"_\"+regressor_label for col in pred_cols]\n",
    "    unscaled_pred_cols = [col+\"_unscaled\" for col in prediction_cols]\n",
    "\n",
    "    # Unscale the prediction columns\n",
    "    df_val[unscaled_cols] = scaler.inverse_transform(df_val[pred_cols])\n",
    "    df_test[unscaled_cols] = scaler.inverse_transform(df_test[pred_cols])\n",
    "    df_val[unscaled_pred_cols] = scaler.inverse_transform(df_val[prediction_cols])\n",
    "    df_test[unscaled_pred_cols] = scaler.inverse_transform(df_test[prediction_cols])\n",
    "\n",
    "    # Set K = sqrt of size of validation set\n",
    "    n_val = len(df_val)\n",
    "    n_test = len(df_test)\n",
    "    n_feat = len(prediction_cols)\n",
    "    k = round(np.sqrt(n_val))\n",
    "\n",
    "    # Get reconstruction errors\n",
    "    reconstruction_cols = [col+\"_reconstruction\"+\"_\"+regressor_label for col in predictors]\n",
    "    valid_re = df_val[predictors].values - df_val[reconstruction_cols].values\n",
    "    test_re = df_test[predictors].values - df_test[reconstruction_cols].values\n",
    "\n",
    "    timer = Timer(seed=seed)\n",
    "    timer.start(description=\"Weighted PI\")\n",
    "\n",
    "    # Mahalanobis distance = euclidan distance after pc with whitening\n",
    "    # - https://stackoverflow.com/questions/69811792/mahalanobis-distance-not-equal-to-euclidean-distance-after-pca\n",
    "    re_scaler = StandardScaler()\n",
    "    pca = PCA(whiten=True)\n",
    "    valid_re = re_scaler.fit_transform(valid_re)\n",
    "    valid_re = pca.fit_transform(valid_re)\n",
    "    test_re = re_scaler.transform(test_re)\n",
    "    test_re = pca.transform(test_re)\n",
    "\n",
    "    # Find neighbours\n",
    "    sigma = 10\n",
    "    kdtree = KDTree(valid_re)\n",
    "    mahalanobis_dist, ind = kdtree.query(test_re, k=k, workers=-1, p=2)\n",
    "    dist = mahalanobis_dist/np.sqrt(n_feat) # already sorted by distance (nearest first; ascending order of distance)\n",
    "    weights = np.exp(-np.square(dist)/(2*sigma**2)) # descending order of weights\n",
    "    print(np.sum(np.sum(weights, axis=1)==0))\n",
    "    print(np.mean(np.var(weights, axis=1)))\n",
    "    modified_alpha = np.ceil((k+1)*(1-alpha))/k\n",
    "\n",
    "    for col in tqdm(pred_cols):\n",
    "        # 1. Val df\n",
    "        # Get error for each variable\n",
    "        val_y = df_val[col+\"_unscaled\"].values.astype('float32')\n",
    "        val_y_pred = df_val[col+pred_label+\"_\"+regressor_label+\"_unscaled\"].values.astype('float32')\n",
    "        val_pe = np.abs(val_y-val_y_pred)\n",
    "\n",
    "        # Nearbouring prediction errors\n",
    "        neighbour_pe = val_pe[ind]\n",
    "\n",
    "        # # PI = 0.95 Weighted Percentile of neigbouring errors\n",
    "        # pis = []\n",
    "        # for i in tqdm(range(n_test), total=n_test, leave=False):\n",
    "        #     df = pd.DataFrame({'v': neighbour_pe[i], 'w': weights[i]})\n",
    "        #     # display(df)\n",
    "        #     calc = wc.Calculator('w')  # w designates weight\n",
    "        #     pi = calc.quantile(df, 'v', modified_alpha)\n",
    "        #     pis.append(pi)\n",
    "        pis = weighted_percentile_matrix(values=neighbour_pe, weights=weights, percentile=modified_alpha)\n",
    "        df_test[col+\"_\"+ue_col+pi_label] = pis\n",
    "\n",
    "    timer.end()\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b7f5b-201c-4d6c-9006-70fb158b1603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ue_dict = {\n",
    "    \"RUE\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \n",
    "    },\n",
    "}    \n",
    "\n",
    "# Apply weighted_prediction_interval to all RUE\n",
    "for time_label, time_info in df_dict.items():\n",
    "    val_df, test_df, pred_cols = time_info[\"valid_df\"], time_info[\"test_df\"], time_info[\"pred_cols\"]\n",
    "    ue_label = \"rue\"\n",
    "    for ue_label, ue_info in ue_dict.items():\n",
    "        pred_label, ue_col = ue_info[\"pred_label\"], ue_info[\"ue_col\"]\n",
    "        df_dict[time_label][\"test_df\"] = weighted_prediction_interval(\n",
    "            df_val=df_dict[time_label][\"valid_df\"], df_test=df_dict[time_label][\"test_df\"], predictors=predictors, pred_cols=pred_cols, \n",
    "            pred_label=pred_label, regressor_label=time_label, ue_col=ue_col, scaler=scaler)\n",
    "\n",
    "save_df_dict(df_dict=df_dict, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25252f3b-8800-4414-a790-6bb1d70a0e83",
   "metadata": {},
   "source": [
    "#### Calculate Conditional Gaussian Prediction Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5c536-a4a3-46b3-a39e-339d80395ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGaussianDistribution:\n",
    "    def __init__(self, Y, X):\n",
    "        self.Y = Y # Assume shape = (num_samples, num_output)\n",
    "        self.X = X # Assume shape = (num_samples, num_inputs)\n",
    "        self.YX = np.concatenate((Y, X), axis=-1) # Assume shape = (num_samples, num_output+num_inputs)\n",
    "        self.num_Y = self.Y.shape[1]\n",
    "        self.num_X = self.X.shape[1]\n",
    "\n",
    "        # Fitting\n",
    "        self.esti_mean_YX = np.mean(self.YX, axis = 0) # (num_output+num_inputs,)\n",
    "        self.esti_covariance_matrix_YX = np.cov(self.YX.T, ddof=1) # (num_output+num_inputs, num_output+num_inputs)\n",
    "        \n",
    "        # Useful stats\n",
    "        self.mu_Y = self.esti_mean_YX[:self.num_Y]\n",
    "        self.mu_X = self.esti_mean_YX[self.num_Y:]\n",
    "        self.cov_XX = self.esti_covariance_matrix_YX[self.num_Y:, self.num_Y:]\n",
    "        self.cov_YY = self.esti_covariance_matrix_YX[:self.num_Y, :self.num_Y]\n",
    "        self.cov_YX = self.esti_covariance_matrix_YX[:self.num_Y, self.num_Y:]\n",
    "        self.cov_XY = self.esti_covariance_matrix_YX[self.num_Y:, :self.num_Y]\n",
    "        \n",
    "    def get_conditional_mean(self, alpha):\n",
    "        # alpha = (num_samples, self.num_X)\n",
    "        return (\n",
    "            self.mu_Y + \n",
    "            np.linalg.multi_dot((\n",
    "                self.cov_YX,\n",
    "                np.linalg.inv(self.cov_XX),\n",
    "                (alpha - self.mu_X).T))\n",
    "        ).flatten()\n",
    "        \n",
    "    def get_conditional_cov(self):\n",
    "        return (\n",
    "            self.cov_YY - \n",
    "            np.linalg.multi_dot((\n",
    "                self.cov_YX,\n",
    "                np.linalg.inv(self.cov_XX), \n",
    "                self.cov_XY))\n",
    "        ).flatten()[0]\n",
    "\n",
    "def cond_gauss_prediction_interval(df_val, df_test, scaler, predictors, pred_cols, pred_label, regressor_label, ue_col, alpha=0.05, seed=seed):\n",
    "    pi_label = \"_cond_gauss\"\n",
    "    df_val, df_test = df_val.copy(), df_test.copy()\n",
    "\n",
    "    unscaled_cols = [col+\"_unscaled\" for col in pred_cols]\n",
    "    prediction_cols = [col+pred_label+\"_\"+regressor_label for col in pred_cols]\n",
    "    unscaled_pred_cols = [col+\"_unscaled\" for col in prediction_cols]\n",
    "\n",
    "    # Unscale the prediction columns\n",
    "    df_val[unscaled_cols] = scaler.inverse_transform(df_val[pred_cols])\n",
    "    df_test[unscaled_cols] = scaler.inverse_transform(df_test[pred_cols])\n",
    "    df_val[unscaled_pred_cols] = scaler.inverse_transform(df_val[prediction_cols])\n",
    "    df_test[unscaled_pred_cols] = scaler.inverse_transform(df_test[prediction_cols])\n",
    "\n",
    "    # Get reconstruction errors\n",
    "    reconstruction_cols = [col+\"_reconstruction_\"+regressor_label for col in predictors]\n",
    "    valid_re = df_val[predictors].values - df_val[reconstruction_cols].values\n",
    "    test_re = df_test[predictors].values - df_test[reconstruction_cols].values\n",
    "\n",
    "    n_val = len(df_val)\n",
    "\n",
    "    timer = Timer(seed=seed)\n",
    "    timer.start(description=\"Conditional PI\")\n",
    "\n",
    "    for col in tqdm(pred_cols):\n",
    "        # 1. Val df\n",
    "        # Get error for each variable\n",
    "        val_y = df_val[col+\"_unscaled\"].values.astype('float32')\n",
    "        val_y_pred = df_val[col+pred_label+\"_\"+regressor_label+\"_unscaled\"].values.astype('float32')\n",
    "        val_pe = val_y-val_y_pred\n",
    "\n",
    "        # Stack both pe and reconstruction error to fit the cond gaussian model\n",
    "        val_data = np.hstack((np.expand_dims(val_pe, axis=1), valid_re))\n",
    "        \n",
    "        # Parameter Estimation on Validation Set\n",
    "        uncertainty_distribution = ConditionalGaussianDistribution(\n",
    "            Y=np.expand_dims(val_pe, axis=1), \n",
    "            X= valid_re\n",
    "        )\n",
    "        esti_conditional_mean_Y = uncertainty_distribution.get_conditional_mean(test_re)\n",
    "        esti_conditional_std_Y = np.sqrt(uncertainty_distribution.get_conditional_cov())\n",
    "        \n",
    "        # 1-alpha/2 -> Because Gaussian is symmetrical\n",
    "        df_test[col+\"_\"+ue_col+pi_label] = norm.ppf(1-alpha/2, loc=esti_conditional_mean_Y, scale=esti_conditional_std_Y).flatten()\n",
    "\n",
    "    timer.end()\n",
    "    \n",
    "    return df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa7c88-a0d4-432a-83ee-d0d2d552334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_dict = {\n",
    "    \"RUE\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \n",
    "    },\n",
    "}    \n",
    "\n",
    "# Apply cond_gauss_prediction_interval to all RUE\n",
    "for time_label, time_info in df_dict.items():\n",
    "    val_df, test_df, pred_cols = time_info[\"valid_df\"], time_info[\"test_df\"], time_info[\"pred_cols\"]\n",
    "    ue_label = \"rue\"\n",
    "    for ue_label, ue_info in ue_dict.items():\n",
    "        pred_label, ue_col = ue_info[\"pred_label\"], ue_info[\"ue_col\"]\n",
    "        df_dict[time_label][\"test_df\"] = cond_gauss_prediction_interval(\n",
    "            df_val=df_dict[time_label][\"valid_df\"], df_test=df_dict[time_label][\"test_df\"], predictors=predictors, pred_cols=pred_cols, \n",
    "            pred_label=pred_label, regressor_label=time_label, ue_col=ue_col, scaler=scaler)\n",
    "\n",
    "save_df_dict(df_dict=df_dict, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e1af6-9a22-4949-9ce0-c365c13bfa2f",
   "metadata": {},
   "source": [
    "#### Calculate Gaussian Corpula Prediction Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337a26b-86de-427d-83ae-542735cdf6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = np.finfo(np.float32).eps\n",
    "\n",
    "class ConditionalGaussianDistribution:\n",
    "    def __init__(self, Y, X):\n",
    "        self.Y = Y # Assume shape = (num_samples, num_output)\n",
    "        self.X = X # Assume shape = (num_samples, num_inputs)\n",
    "        self.YX = np.concatenate((Y, X), axis=-1) # Assume shape = (num_samples, num_output+num_inputs)\n",
    "        self.num_Y = self.Y.shape[1]\n",
    "        self.num_X = self.X.shape[1]\n",
    "\n",
    "        # Fitting\n",
    "        self.esti_mean_YX = np.mean(self.YX, axis = 0) # (num_output+num_inputs,)\n",
    "        self.esti_covariance_matrix_YX = np.cov(self.YX.T, ddof=1) # (num_output+num_inputs, num_output+num_inputs)\n",
    "        \n",
    "        # Useful stats\n",
    "        self.mu_Y = self.esti_mean_YX[:self.num_Y]\n",
    "        self.mu_X = self.esti_mean_YX[self.num_Y:]\n",
    "        self.cov_XX = self.esti_covariance_matrix_YX[self.num_Y:, self.num_Y:]\n",
    "        self.cov_YY = self.esti_covariance_matrix_YX[:self.num_Y, :self.num_Y]\n",
    "        self.cov_YX = self.esti_covariance_matrix_YX[:self.num_Y, self.num_Y:]\n",
    "        self.cov_XY = self.esti_covariance_matrix_YX[self.num_Y:, :self.num_Y]\n",
    "        \n",
    "    def get_conditional_mean(self, alpha):\n",
    "        # alpha = (num_samples, self.num_X)\n",
    "        return (\n",
    "            self.mu_Y + \n",
    "            np.linalg.multi_dot((\n",
    "                self.cov_YX,\n",
    "                np.linalg.inv(self.cov_XX),\n",
    "                (alpha - self.mu_X).T))\n",
    "        ).flatten()\n",
    "        \n",
    "    def get_conditional_cov(self):\n",
    "        return (\n",
    "            self.cov_YY - \n",
    "            np.linalg.multi_dot((\n",
    "                self.cov_YX,\n",
    "                np.linalg.inv(self.cov_XX), \n",
    "                self.cov_XY))\n",
    "        ).flatten()[0]\n",
    "\n",
    "class GaussianCopula:\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        self.n_feat = X.shape[1]\n",
    "\n",
    "        # Get all CDFs\n",
    "        self.cdfs = []\n",
    "        for i_feat in range(self.n_feat):\n",
    "            self.cdfs.append(stats.ecdf(X[:, i_feat]))\n",
    "\n",
    "    def add_y(self, y):\n",
    "        self.y = y\n",
    "        self.y_cdf = stats.ecdf(self.y)\n",
    "        \n",
    "    def X_to_V(self, cur_X):\n",
    "        X_Vi = np.empty(cur_X.shape)\n",
    "        for i_feat in range(self.n_feat):\n",
    "            Fi = self.cdfs[i_feat].cdf.evaluate(cur_X[:, i_feat]).clip(EPSILON, 1 - EPSILON)\n",
    "            X_Vi[:,i_feat] = norm.ppf(Fi)\n",
    "        return X_Vi\n",
    "\n",
    "    def Y_to_V(self, cur_y):\n",
    "        y_Fi = self.y_cdf.cdf.evaluate(cur_y).clip(EPSILON, 1 - EPSILON)\n",
    "        y_Vi = norm.ppf(y_Fi)\n",
    "        return y_Vi\n",
    "\n",
    "    def V_to_Y(self, cur_y_Vi):\n",
    "        y_Fi = norm.cdf(cur_y_Vi)\n",
    "        y = np.percentile(self.y, y_Fi*100, axis=0, method=\"higher\")\n",
    "        return y\n",
    "\n",
    "def gauss_copula_prediction_interval(df_val, df_test, scaler, predictors, pred_cols, pred_label, regressor_label, ue_col, alpha=0.05, seed=seed):\n",
    "    pi_label = \"_gauss_copula\"\n",
    "    df_val, df_test = df_val.copy(), df_test.copy()\n",
    "\n",
    "    unscaled_cols = [col+\"_unscaled\" for col in pred_cols]\n",
    "    prediction_cols = [col+pred_label+\"_\"+regressor_label for col in pred_cols]\n",
    "    unscaled_pred_cols = [col+\"_unscaled\" for col in prediction_cols]\n",
    "\n",
    "    # Unscale the prediction columns\n",
    "    df_val[unscaled_cols] = scaler.inverse_transform(df_val[pred_cols])\n",
    "    df_test[unscaled_cols] = scaler.inverse_transform(df_test[pred_cols])\n",
    "    df_val[unscaled_pred_cols] = scaler.inverse_transform(df_val[prediction_cols])\n",
    "    df_test[unscaled_pred_cols] = scaler.inverse_transform(df_test[prediction_cols])\n",
    "\n",
    "    # Get reconstruction errors\n",
    "    reconstruction_cols = [col+\"_reconstruction\"+\"_\"+regressor_label for col in predictors]\n",
    "    valid_re = df_val[predictors].values - df_val[reconstruction_cols].values\n",
    "    test_re = df_test[predictors].values - df_test[reconstruction_cols].values\n",
    "\n",
    "    timer = Timer(seed=seed)\n",
    "    timer.start(description=\"Copula PI\")\n",
    "\n",
    "    # Convert reconstruction and prediction error to V\n",
    "    gc = GaussianCopula(valid_re)\n",
    "    valid_re = gc.X_to_V(valid_re)\n",
    "    test_re = gc.X_to_V(test_re)\n",
    "\n",
    "    n_val = len(df_val)\n",
    "\n",
    "    for col in tqdm(pred_cols):\n",
    "        # 1. Val df\n",
    "        # Get error for each variable\n",
    "        val_y = df_val[col+\"_unscaled\"].values.astype('float32')\n",
    "        val_y_pred = df_val[col+pred_label+\"_\"+regressor_label+\"_unscaled\"].values.astype('float32')\n",
    "        val_pe = np.abs(val_y-val_y_pred)\n",
    "\n",
    "        gc.add_y(val_pe)\n",
    "        val_pe = gc.Y_to_V(val_pe)\n",
    "\n",
    "        # Parameter Estimation on Validation Set\n",
    "        uncertainty_distribution = ConditionalGaussianDistribution(\n",
    "            Y=np.expand_dims(val_pe, axis=1), \n",
    "            X= valid_re\n",
    "        )\n",
    "        esti_conditional_mean_Y = uncertainty_distribution.get_conditional_mean(test_re)\n",
    "        esti_conditional_std_Y = np.sqrt(uncertainty_distribution.get_conditional_cov())\n",
    "\n",
    "        pi_V = norm.ppf(1-alpha, loc=esti_conditional_mean_Y, scale=esti_conditional_std_Y).flatten()\n",
    "        # print(pi_V)\n",
    "\n",
    "        # Convert from V space back to Y\n",
    "        df_test[col+\"_\"+ue_col+pi_label] = gc.V_to_Y(pi_V)\n",
    "\n",
    "    timer.end()\n",
    "    \n",
    "    return df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807c8be-f6ed-4e39-a8a5-8e5e7444e975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ue_dict = {\n",
    "    \"RUE\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \n",
    "    },\n",
    "}    \n",
    "\n",
    "# Apply weighted_prediction_interval to all RUE\n",
    "for time_label, time_info in df_dict.items():\n",
    "    val_df, test_df, pred_cols = time_info[\"valid_df\"], time_info[\"test_df\"], time_info[\"pred_cols\"]\n",
    "    ue_label = \"rue\"\n",
    "    for ue_label, ue_info in ue_dict.items():\n",
    "        pred_label, ue_col = ue_info[\"pred_label\"], ue_info[\"ue_col\"]\n",
    "        df_dict[time_label][\"test_df\"] = gauss_copula_prediction_interval(\n",
    "            df_val=df_dict[time_label][\"valid_df\"], df_test=df_dict[time_label][\"test_df\"], predictors=predictors, pred_cols=pred_cols, \n",
    "            pred_label=pred_label, regressor_label=time_label, ue_col=ue_col, scaler=scaler)\n",
    "\n",
    "save_df_dict(df_dict=df_dict, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ca72f-2bce-4e2b-bea4-a104288b3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = load_df_dict(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c56d1-5c5c-4052-af2d-6f4b43603720",
   "metadata": {},
   "source": [
    "#### Calculate Conformal Prediction Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12486728-a52d-4ff6-a287-25d0ee821754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_prediction_interval(df_val, df_test, scaler, predictors, pred_cols, pred_label, regressor_label, ue_col, alpha=0.05, seed=seed):\n",
    "    pi_label = \"_conformal\"\n",
    "    df_val, df_test = df_val.copy(), df_test.copy()\n",
    "    unscaled_cols = [col+\"_unscaled\" for col in pred_cols]\n",
    "    prediction_cols = [col+pred_label+\"_\"+regressor_label for col in pred_cols]\n",
    "    unscaled_pred_cols = [col+\"_unscaled\" for col in prediction_cols]\n",
    "    df_val[unscaled_cols] = scaler.inverse_transform(df_val[pred_cols])\n",
    "    df_test[unscaled_cols] = scaler.inverse_transform(df_test[pred_cols])\n",
    "    df_val[unscaled_pred_cols] = scaler.inverse_transform(df_val[prediction_cols])\n",
    "    df_test[unscaled_pred_cols] = scaler.inverse_transform(df_test[prediction_cols])\n",
    "\n",
    "    timer = Timer(seed=seed)\n",
    "    timer.start(description=\"Conformal PI\")\n",
    "\n",
    "    for col in pred_cols:\n",
    "        # 1. Val df\n",
    "        # Get error for each variable\n",
    "        val_y = df_val[col+\"_unscaled\"].values.astype('float32')\n",
    "        val_y_pred = df_val[col+pred_label+\"_\"+regressor_label+\"_unscaled\"].values.astype('float32')\n",
    "        val_error = np.abs(val_y-val_y_pred)\n",
    "    \n",
    "        # Get uncertainty \n",
    "        val_ue = df_val[ue_col].astype('float32')\n",
    "\n",
    "        # Get scores\n",
    "        val_scores = val_error/val_ue\n",
    "        # Get the score quantile\n",
    "        n = len(df_val)\n",
    "        qhat = np.quantile(val_scores, np.ceil((n+1)*(1-alpha))/n, method='higher')\n",
    "\n",
    "        # 2. Test df\n",
    "        # Get uncertainty \n",
    "        test_ue = df_test[ue_col].astype('float32')\n",
    "\n",
    "        # Caclulate PI\n",
    "        df_test[col+\"_\"+ue_col+pi_label] = test_ue*qhat\n",
    "        \n",
    "    timer.end()\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7228a-610e-4981-8cdc-c08b8372cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_dict = {\n",
    "    \"RUE\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \n",
    "    },\n",
    "    \"MC Dropout\": {\n",
    "        \"pred_label\": \"_mean\", \"ue_col\": \"mcd\", \n",
    "    },\n",
    "    \"GPR\": {\n",
    "        \"pred_label\": \"_gpr\", \"ue_col\": \"gpr_std_mean\", \n",
    "    },\n",
    "    \"Infer-Noise\": {\n",
    "        \"pred_label\": \"_infernoise\", \"ue_col\": \"infernoise_uncertainty\", \n",
    "    },\n",
    "    \"BNN\": {\n",
    "        \"pred_label\": \"_bnn\", \"ue_col\": \"bnn_uncertainty\", \n",
    "    },\n",
    "    \"DER\": {\n",
    "        \"pred_label\": \"_der\", \"ue_col\": \"der_uncertainty\", \n",
    "    }\n",
    "}    \n",
    "\n",
    "# Apply conformal prediction to all UEs\n",
    "for time_label, time_info in df_dict.items():\n",
    "    val_df, test_df, pred_cols = time_info[\"valid_df\"], time_info[\"test_df\"], time_info[\"pred_cols\"]\n",
    "    for ue_label, ue_info in ue_dict.items():\n",
    "        pred_label, ue_col = ue_info[\"pred_label\"], ue_info[\"ue_col\"]\n",
    "        df_dict[time_label][\"test_df\"] = conformal_prediction_interval(\n",
    "            df_val=df_dict[time_label][\"valid_df\"], df_test=df_dict[time_label][\"test_df\"], predictors=predictors, pred_cols=pred_cols, \n",
    "            pred_label=pred_label, regressor_label=time_label, ue_col=ue_col, scaler=scaler)\n",
    "\n",
    "save_df_dict(df_dict=df_dict, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cb025-ad59-4f40-a320-53aee9f1b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[\"t+1\"][\"test_df\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c1874",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e3813-315a-470a-a910-4397368ae0f5",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c154b-be10-4942-849b-7c2fcedcdc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_performance_table(test_df_dict, ue_dict):\n",
    "    perf_df_dict = []\n",
    "    for ue_name, ue_info in ue_dict.items():\n",
    "        ue_row_dict = {\"Model\": ue_name}\n",
    "        pred_label = ue_info[\"pred_label\"]\n",
    "        for regressor_label, test_df_info in test_df_dict.items():\n",
    "            test_df = test_df_info[\"test_df\"]\n",
    "            pred_cols = test_df_info[\"pred_cols\"]\n",
    "            y_pred_cols = [col+pred_label+\"_\"+regressor_label for col in pred_cols]\n",
    "            # ue_cols = [col+ue_label for col in predictors]\n",
    "            ue_row_dict[regressor_label] = sklearn.metrics.mean_squared_error(test_df[pred_cols], test_df[y_pred_cols])\n",
    "        perf_df_dict.append(ue_row_dict)\n",
    "    return pd.DataFrame(perf_df_dict)\n",
    "\n",
    "def calculate_aurc(ue, loss):\n",
    "    num_samples = len(ue)\n",
    "    ue_loss_df = pd.DataFrame({\"ue\":ue, \"loss\":loss})\n",
    "    ue_loss_df = ue_loss_df.sort_values(by=\"ue\", ascending=True)\n",
    "    ue_loss_df[\"cumulative_loss\"] = ue_loss_df[\"loss\"].expanding().mean()\n",
    "    ue_loss_df[\"coverage\"] = (np.arange(num_samples)+1)/num_samples\n",
    "    return auc(ue_loss_df[\"coverage\"], ue_loss_df[\"cumulative_loss\"].values)\n",
    "\n",
    "def min_max_norm(vec):\n",
    "    return (vec - vec.min()) / (vec.max() - vec.min())\n",
    "\n",
    "def remove_outliers(vec):\n",
    "    # vector within 3 std away from mean\n",
    "    # data_mean, data_std = np.mean(vec), np.std(vec)\n",
    "    # num_std = 3\n",
    "    # return vec[(vec <= data_mean + num_std*data_std) & (vec >= data_mean - num_std*data_std)]\n",
    "\n",
    "    Q1 = np.percentile(vec, 25, method= 'midpoint') \n",
    "    Q3 = np.percentile(vec, 75, method= 'midpoint') \n",
    "    IQR = Q3 - Q1 \n",
    "    # low_lim = Q1 - 1.5 * IQR\n",
    "    up_lim = Q3 + 1.5 * IQR\n",
    "    return vec[(vec <= up_lim )]\n",
    "    \n",
    "def min_max_norm_wo_outliers(vec):\n",
    "    vec_wo_outliers = remove_outliers(vec)\n",
    "    vec_min, vec_max = min(vec_wo_outliers), max(vec_wo_outliers)\n",
    "    return (vec - vec_min) / (vec_max - vec_min)\n",
    "\n",
    "def calculate_sigma_risk(ue, loss, thres):\n",
    "    ue = min_max_norm_wo_outliers(ue)\n",
    "    return loss[ue<=thres].mean()\n",
    "\n",
    "def get_ue_performance_table(test_df_dict, ue_dict):\n",
    "    from scipy.stats import pearsonr\n",
    "    perf_df_dict = []\n",
    "    for ue_name, ue_info in ue_dict.items():\n",
    "        ue_row_dict = {\"Model\": ue_name}\n",
    "        pred_label = ue_info[\"pred_label\"]\n",
    "        ue_col = ue_info[\"ue_col\"]\n",
    "        for regressor_label, test_df_info in test_df_dict.items():\n",
    "            test_df = test_df_info[\"test_df\"]\n",
    "            pred_cols = test_df_info[\"pred_cols\"]\n",
    "            y_pred_cols = [col+pred_label+\"_\"+regressor_label for col in pred_cols]\n",
    "            \n",
    "            y_true = test_df[pred_cols].values\n",
    "            y_pred = test_df[y_pred_cols].values\n",
    "            ues = test_df[ue_col].values\n",
    "            \n",
    "            mean_abs_errors = np.mean(np.abs(y_true-y_pred), axis=1)\n",
    "            \n",
    "            corr, p_value = pearsonr(ues, mean_abs_errors)\n",
    "            aurc = calculate_aurc(ues, mean_abs_errors)\n",
    "\n",
    "            ue_row_dict[regressor_label+\" Corr\"] = corr\n",
    "            ue_row_dict[regressor_label+\" Pval\"] = p_value\n",
    "            ue_row_dict[regressor_label+\" AURC\"] = aurc\n",
    "\n",
    "            for thres in [0.1, 0.2, 0.3, 0.4]:\n",
    "                ue_row_dict[regressor_label+f\" Sigma={thres}\"] = calculate_sigma_risk(ues, mean_abs_errors, thres)\n",
    "            \n",
    "        perf_df_dict.append(ue_row_dict)\n",
    "    return pd.DataFrame(perf_df_dict)\n",
    "\n",
    "def calculate_picp_for_a_feature(actual, lb, ub):\n",
    "    # PICP (Prediction interval coverage probability)\n",
    "    # - The percentage of points within the prediction interval; The higher the better. \n",
    "    points_within_pi = ((actual >= lb) & (actual <= ub))\n",
    "    num_points_within_pi = points_within_pi.sum()\n",
    "    total_num_points = len(actual)\n",
    "    picp = num_points_within_pi/total_num_points\n",
    "    return picp\n",
    "\n",
    "def calculate_pinaw_for_a_feature(actual, lb, ub):\n",
    "    # Prediction interval normalized average width (PINAW)\n",
    "    # - The average size of the prediction interval. \n",
    "    range_of_underlying_target = actual.max() - actual.min()\n",
    "    total_num_points = len(actual)\n",
    "    width_of_pi = ub-lb\n",
    "    sum_of_widths_of_pi = width_of_pi.sum()\n",
    "    pinaw = sum_of_widths_of_pi/(total_num_points*range_of_underlying_target)\n",
    "    return pinaw\n",
    "\n",
    "def calculate_pinafd_for_a_feature(actual, lb, ub):\n",
    "    # Prediction interval normalized average failure distance (PINAFD)\n",
    "    # - The average distance of points outside of the prediction interval to the bounds of the prediction interval. \n",
    "    range_of_underlying_target = actual.max() - actual.min()\n",
    "    points_outside_pi = ((actual < lb) | (actual > ub))\n",
    "    num_points_outside_pi = points_outside_pi.sum()\n",
    "    if num_points_outside_pi == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        dist_lb = (lb-actual).abs()\n",
    "        dist_ub = (ub-actual).abs()\n",
    "        shortest_dist_to_any_b = np.minimum(dist_lb, dist_ub)\n",
    "        failure_dist = shortest_dist_to_any_b[points_outside_pi]\n",
    "        pinafd = failure_dist.sum()/(num_points_outside_pi*range_of_underlying_target)\n",
    "        return pinafd\n",
    "    \n",
    "def calculate_cp_for_a_feature(actual, lb, ub):\n",
    "    picp = calculate_picp_for_a_feature(actual, lb, ub)\n",
    "    alpha = 0.05\n",
    "    delta = alpha/50\n",
    "    cp = (1-alpha+delta-picp)**2\n",
    "    return cp\n",
    "    \n",
    "    \n",
    "def calculate_cwfdc_for_a_feature(actual, lb, ub):\n",
    "    pinaw = calculate_pinaw_for_a_feature(actual, lb, ub)\n",
    "    pinafd = calculate_pinafd_for_a_feature(actual, lb, ub)\n",
    "    cp = calculate_cp_for_a_feature(actual, lb, ub)\n",
    "    rho = 1\n",
    "    beta = 1000\n",
    "    cwfdc = pinaw + rho*pinafd + beta * cp\n",
    "    return cwfdc\n",
    "\n",
    "def aggregate_metrics_across_feats(df_info, pi_info, time_label, metric_func):\n",
    "    total_metric = 0\n",
    "    pred_cols, df_test = df_info[\"pred_cols\"], df_info[\"test_df\"]\n",
    "    pred_label, ue_col,  pi_label = pi_info[\"pred_label\"], pi_info[\"ue_col\"], pi_info[\"pi_label\"]\n",
    "    num_feat = len(pred_cols)\n",
    "    for pred_col in pred_cols:\n",
    "        pi_col = f\"{pred_col}_{ue_col}{pi_label}\"\n",
    "        predicted_col = f\"{pred_col}{pred_label}_{time_label}_unscaled\"\n",
    "        pi, pred = df_test[pi_col], df_test[predicted_col]\n",
    "        total_metric += metric_func(\n",
    "            actual=df_test[pred_col+\"_unscaled\"], \n",
    "            lb=pred-pi, \n",
    "            ub=pred+pi\n",
    "        )\n",
    "    av_metric = total_metric/num_feat\n",
    "    return av_metric\n",
    "\n",
    "def calculate_metrics(df_dict, pi_dict):\n",
    "    output_df_list = []\n",
    "    feature_list = ['SpO2 (%)', 'RESP (bpm)', 'ABPmean (mmHg)', 'HR (bpm)', 'ABPsys (mmHg)', 'ABPdias (mmHg)']\n",
    "    for time_label, df_info in df_dict.items():\n",
    "        for pi_label, pi_info in pi_dict.items():\n",
    "            picp = aggregate_metrics_across_feats(df_info, pi_info, time_label, metric_func=calculate_picp_for_a_feature)\n",
    "            pinaw = aggregate_metrics_across_feats(df_info, pi_info, time_label, metric_func=calculate_pinaw_for_a_feature)\n",
    "            pinafd = aggregate_metrics_across_feats(df_info, pi_info, time_label, metric_func=calculate_pinafd_for_a_feature)\n",
    "            cp = aggregate_metrics_across_feats(df_info, pi_info, time_label, metric_func=calculate_cp_for_a_feature)\n",
    "            cwfdc = aggregate_metrics_across_feats(df_info, pi_info, time_label, metric_func=calculate_cwfdc_for_a_feature)\n",
    "            output_df_list.append({\n",
    "                \"Time Horizon\":time_label,\n",
    "                \"Method\":pi_label,\n",
    "                \"PICP\":picp,\n",
    "                \"PINAW\":pinaw,\n",
    "                \"PINAFD\":pinafd,\n",
    "                \"CP\":cp,\n",
    "                \"CWFDC\":cwfdc\n",
    "            })\n",
    "            \n",
    "    output_df = pd.DataFrame(output_df_list)\n",
    "    return output_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dcbad5-425f-4a13-80d8-54059b08c6f0",
   "metadata": {},
   "source": [
    "### Load Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scaler(fp_downsampled_scaler_file):\n",
    "    import pickle \n",
    "    with open(fp_downsampled_scaler_file, 'rb') as handle:\n",
    "        scaler = pickle.load(handle)\n",
    "    return scaler\n",
    "\n",
    "# Load all predictions\n",
    "def load_df_dict(seed, time_labels=[\"t+1\", \"t+2\", \"t+3\"]):\n",
    "    df_dict = {}\n",
    "    for time_label in tqdm(time_labels):\n",
    "        fp_seed_folder = os.path.join(fp_project_pi_predictions, str(seed))\n",
    "        val_df = pd.read_csv(os.path.join(fp_seed_folder, \"val_\"+time_label+\".csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(fp_seed_folder, \"test_\"+time_label+\".csv\"))\n",
    "        pred_cols = joblib.load(os.path.join(fp_seed_folder, \"pred_cols_\"+time_label+\".joblib\"))\n",
    "        df_dict[time_label] = {\"valid_df\": val_df, \"test_df\": test_df, \"pred_cols\":pred_cols}\n",
    "    print(\"Loaded df_dict!\")\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133c984-7114-4481-b65d-dab59d8a6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = load_scaler(fp_downsampled_scaler_file)\n",
    "\n",
    "df_dict = load_df_dict(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f10836-044f-447b-830a-3b8203a78771",
   "metadata": {},
   "source": [
    "### Prediction Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36cbc8-f7f1-4333-a396-eb8f5357366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_dict = {\n",
    "    \"RUE\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \n",
    "    },\n",
    "    \"Infer-Noise\": {\n",
    "        \"pred_label\": \"_infernoise\", \"ue_col\": \"infernoise_uncertainty\", \n",
    "    },\n",
    "    \"MC Dropout\": {\n",
    "        \"pred_label\": \"_mean\", \"ue_col\": \"mcd\", \n",
    "    },\n",
    "    \"GPR\": {\n",
    "        \"pred_label\": \"_gpr\", \"ue_col\": \"gpr_std_mean\", \n",
    "    },\n",
    "    \"BNN\": {\n",
    "        \"pred_label\": \"_bnn\", \"ue_col\": \"bnn_uncertainty\", \n",
    "    },\n",
    "    \"DER\": {\n",
    "        \"pred_label\": \"_der\", \"ue_col\": \"der_uncertainty\", \n",
    "    },\n",
    "}    \n",
    "pred_perf_df = get_prediction_performance_table(df_dict, ue_dict)\n",
    "display(pred_perf_df)\n",
    "pred_perf_df.to_csv(os.path.join(fp_project_model_evaluations, str(seed), \"pred_perf.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c952fd4-9e90-4365-a0cc-fc688103b2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T09:02:52.450657Z",
     "iopub.status.busy": "2024-08-15T09:02:52.450354Z",
     "iopub.status.idle": "2024-08-15T09:02:52.453002Z",
     "shell.execute_reply": "2024-08-15T09:02:52.452642Z",
     "shell.execute_reply.started": "2024-08-15T09:02:52.450644Z"
    }
   },
   "source": [
    "### Compare UE Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101330e5-bf00-4825-933f-d7c49cdb29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_perf_df = get_ue_performance_table(df_dict, ue_dict)\n",
    "display(ue_perf_df)\n",
    "ue_perf_df.to_csv(os.path.join(fp_project_model_evaluations, str(seed), \"ue_perf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663f374-a361-422f-82df-a7c9c06b4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_ue_perf(ue_perf_df):\n",
    "    for i in range(3):\n",
    "        column_indices = [0] + list(range(1+i*7, 1+(i+1)*7))\n",
    "        display(ue_perf_df.iloc[:,column_indices].set_index(\"Model\"))\n",
    "display_ue_perf(ue_perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adefd0-a1af-45bc-9fb2-2cf5180c45d7",
   "metadata": {},
   "source": [
    "### Compare PI Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f232304-664a-484c-86da-8d8b68e74bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_dict = {\n",
    "    \"RUE Gaussian Copula\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_gauss_copula\"\n",
    "    },\n",
    "    \"RUE Conditional Gaussian\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_cond_gauss\"\n",
    "    },\n",
    "    \"RUE Weighted\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_weighted\"\n",
    "    },\n",
    "    \"RUE KNN\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_knn\"\n",
    "    },\n",
    "    \"RUE Conformal\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_conformal\"\n",
    "    },\n",
    "    \"Infer-Noise Conformal\": {\n",
    "        \"pred_label\": \"_infernoise\", \"ue_col\": \"infernoise_uncertainty\", \"pi_label\": \"_conformal\"\n",
    "    },\n",
    "    \"MC Dropout Conformal\": {\n",
    "        \"pred_label\": \"_mean\", \"ue_col\": \"mcd\", \"pi_label\": \"_conformal\"\n",
    "    },\n",
    "    \"GPR Conformal\": {\n",
    "        \"pred_label\": \"_gpr\", \"ue_col\": \"gpr_std_mean\", \"pi_label\": \"_conformal\"\n",
    "    },\n",
    "    \"BNN Conformal\": {\n",
    "        \"pred_label\": \"_bnn\", \"ue_col\": \"bnn_uncertainty\", \"pi_label\": \"_conformal\"\n",
    "    },\n",
    "    \"DER Conformal\": {\n",
    "        \"pred_label\": \"_der\", \"ue_col\": \"der_uncertainty\", \"pi_label\": \"_conformal\"\n",
    "    },\n",
    "}   \n",
    "\n",
    "pi_stats = calculate_metrics(df_dict, pi_dict)\n",
    "# pi_stats.to_csv(os.path.join(fp_project_model_evaluations, str(seed), \"pi_perf.csv\"))\n",
    "pi_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9669c-bbc8-4bef-8a9b-3686f9196ce7",
   "metadata": {},
   "source": [
    "### Paper Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178e71c-0b51-4a39-8d1c-0d19dae43406",
   "metadata": {},
   "source": [
    "#### Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda7203-d807-4b2f-8fae-87004030b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_with_pi(df_dict, pi_dict, scaler, seed, record=\"055n\", dpi=300, save_figs=False):\n",
    "    fp_fig_folder = os.path.join(fp_project_model_evaluations, str(seed), \"pi_line_graphs\")\n",
    "    create_folder(fp_fig_folder)\n",
    "\n",
    "    features = scaler.feature_names_in_\n",
    "\n",
    "    num_rows = len(df_dict)\n",
    "    num_cols = len(df_dict[\"t+1\"][\"pred_cols\"])\n",
    "    \n",
    "    # Plots in multiples of three\n",
    "    for pi_name, pi_info in pi_dict.items():\n",
    "        print(pi_name, \":\")\n",
    "        pred_label, ue_col, pi_label = pi_info[\"pred_label\"], pi_info[\"ue_col\"], pi_info[\"pi_label\"]\n",
    "\n",
    "        # Plot for all three time intervals\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, dpi=dpi, figsize=(2.5*num_cols, 2*num_rows))\n",
    "        \n",
    "        for i, (regressor_label, test_df_info) in enumerate(df_dict.items()):\n",
    "            test_df = test_df_info[\"test_df\"]\n",
    "            test_record_df = test_df.loc[test_df[\"record\"]==record]\n",
    "            pred_cols = test_df_info[\"pred_cols\"]\n",
    "\n",
    "            #     # Sort columns for display\n",
    "            zipped_cols = list(zip(features, pred_cols))\n",
    "            zipped_cols = sorted(zipped_cols, key = lambda x: x[0])\n",
    "    \n",
    "            for j, (feature, pred_col) in enumerate(zipped_cols):\n",
    "                y_pred_col = pred_col+pred_label+\"_\"+regressor_label\n",
    "\n",
    "                index = test_record_df[\"target_index\"]\n",
    "                y_true = test_record_df[pred_col+\"_unscaled\"].values\n",
    "                y_pred = test_record_df[y_pred_col+\"_unscaled\"].values\n",
    "                pi = test_record_df[pred_col+\"_\"+ue_col+pi_label].values\n",
    "                \n",
    "                # Plot predictions and their CI\n",
    "                axes[i,j].plot(index, y_true, color=\"red\")\n",
    "                axes[i,j].plot(index, y_pred, color=\"green\")\n",
    "                axes[i,j].fill_between(\n",
    "                    index, (y_pred-pi), (y_pred+pi), \n",
    "                    color='green', alpha=0.3\n",
    "                )  \n",
    "                if j!=0:\n",
    "                    axes[i,j].set_ylabel(feature)\n",
    "                else:\n",
    "                    axes[i,j].set_ylabel(regressor_label+\"\\n\"+feature)\n",
    "        fig.suptitle(pi_name)\n",
    "        plt.tight_layout()\n",
    "        if save_figs:\n",
    "            plt.savefig(os.path.join(fp_fig_folder, ue_col+pi_label+\".jpg\"), bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f1ab5-6c11-425f-81b8-7f64de5cf60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to calculate pis\n",
    "plot_predictions_with_pi(df_dict, pi_dict, scaler, seed=seed, dpi=128, save_figs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424754a-e3cf-486a-8c0d-89a5a2df7f50",
   "metadata": {},
   "source": [
    "#### Parallel Coordinate Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d49f8-945a-49db-b1a4-4f70ddff6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pi_metrics_parallel_coordinate(pi_stats, seed, categories=[\"CP\", \"PINAW\", \"PINAFD\"], save_fig=False):\n",
    "    from matplotlib import cm\n",
    "    from matplotlib.colors import rgb2hex\n",
    "    import seaborn as sns\n",
    "    \n",
    "    pi_stats = pi_stats.copy().sort_values(by=[\"Time Horizon\", \"Method\"])\n",
    "    num_cat = len(categories)\n",
    "    \n",
    "    rue_colors = sns.color_palette('spring', n_colors=4)\n",
    "    other_colors = sns.color_palette('winter', n_colors=3)\n",
    "    \n",
    "    # color_list =  other_colors + rue_colors\n",
    "    color_list = sns.color_palette('rainbow', n_colors=pi_stats[\"Method\"].nunique())\n",
    "    print(pi_stats[\"Method\"].unique())\n",
    "    # color_list = [\n",
    "    #     '#1f77b4',  # muted blue\n",
    "    #     '#ff7f0e',  # safety orange\n",
    "    #     '#2ca02c',  # cooked asparagus green\n",
    "    #     '#d62728',  # brick red\n",
    "    #     '#9467bd',  # muted purple\n",
    "    #     '#8c564b',  # chestnut brown\n",
    "    #     '#e377c2',  # raspberry yogurt pink\n",
    "    # #     '#7f7f7f',  # middle gray\n",
    "    # #     '#bcbd22',  # curry yellow-green\n",
    "    # #     '#17becf'   # blue-teal\n",
    "    # ]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_cat, sharey=True, dpi=300, figsize=(num_cat*3, 3))\n",
    "    xticks = range(num_cat)\n",
    "    \n",
    "    for i, (time_horizon_label, time_df) in enumerate(pi_stats.groupby(\"Time Horizon\")):\n",
    "        time_df[categories] = (time_df[categories]-time_df[categories].min())/(time_df[categories].max()-time_df[categories].min())\n",
    "        for j, (method_label, method_df) in enumerate(time_df.groupby(\"Method\")):\n",
    "            # print(method_df[categories].values.flatten())\n",
    "            axes[i].plot(xticks, method_df[categories].values.flatten(), label=method_label if i==0 else None, linestyle=\"--\" if \"RUE\" not in method_label else \"-\")\n",
    "            axes[i].set_xticks(xticks, categories)\n",
    "        axes[i].set_title(time_horizon_label)\n",
    "\n",
    "    reorder=lambda hl,nc:(sum((lis[i::nc]for i in range(nc)),[])for lis in hl)\n",
    "    h_l = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(*reorder(h_l, 5), loc='upper center', bbox_to_anchor=(0.5, 0), ncol=5, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    if save_fig:\n",
    "        fp_fig_folder = os.path.join(fp_project_model_evaluations, str(seed))\n",
    "        plt.savefig(os.path.join(fp_fig_folder, \"parallel_coord.jpg\"), bbox_inches=\"tight\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3765c94-8ef2-4b27-8745-55977274a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pi_metrics_parallel_coordinate(pi_stats, seed=seed, categories=[\"CP\", \"PINAW\", \"PINAFD\"], save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5ac31-af89-4e03-a5a1-131ed7fc8e72",
   "metadata": {},
   "source": [
    "#### UE-Error Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144b6f3-a7ae-4945-88d1-bc7d6b0af820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subtitle(fig: plt.Figure, grid: SubplotSpec, title: str):\n",
    "    \"Sign sets of subplots with title\"\n",
    "    row = fig.add_subplot(grid)\n",
    "    # the '\\n' is important\n",
    "    row.set_title(f'{title}\\n', fontweight='semibold', pad=0, y=0.975)\n",
    "    # hide subplot\n",
    "    row.set_frame_on(False)\n",
    "    row.axis('off')\n",
    "\n",
    "# Scatter plot of rows of t+1, t+2... t+3\n",
    "# - In each row, we have the scatterplots of each UE\n",
    "def get_ue_loss_scatterplot(test_df_dict, ue_dict, seed, nbins=10, dpi=300, save_fig=False):\n",
    "    eqn_label_fs = 7\n",
    "    line_col = 'black'\n",
    "    point_color = \"#0090C1\"\n",
    "    point_size = 75\n",
    "    point_alpha = 0.5\n",
    "    marker = \".\"\n",
    "    formatting_dict = dict(color=point_color, s=point_size, alpha=point_alpha, marker=marker, edgecolors='none')\n",
    "    \n",
    "    bin_width = int(100/nbins)\n",
    "    num_cols = len(ue_dict)\n",
    "    num_rows = len(test_df_dict) # num intervals\n",
    "    #, constrained_layout=True\n",
    "    # fig, axes = plt.subplots(num_rows*2, num_cols, dpi=dpi, figsize=(num_cols*4, 1.5*2*num_rows)) # , sharey=\"row\", sharex=\"col\"\n",
    "    fig = plt.figure(dpi=dpi, figsize=(num_cols*3, 2*2*num_rows))\n",
    "    grid = fig.add_gridspec(num_rows, 1, height_ratios=[1 for i in range(num_rows)], hspace=0.3)\n",
    "    \n",
    "    # For all time intervals \n",
    "    for time_i ,(regressor_label, test_df_info) in enumerate(test_df_dict.items()):\n",
    "        cur_gs = grid[time_i].subgridspec(2, num_cols, wspace=0, hspace=0.1)\n",
    "        axes = cur_gs.subplots(sharey='row', sharex='col')\n",
    "        test_df = test_df_info[\"test_df\"]\n",
    "        pred_cols = test_df_info[\"pred_cols\"]\n",
    "        y_true = test_df[pred_cols].values\n",
    "\n",
    "        create_subtitle(fig, grid[time_i, ::], regressor_label)\n",
    "        # For each ue \n",
    "        for ue_i, (ue_type, ue_info) in enumerate(ue_dict.items()):\n",
    "            \n",
    "            ue_df = test_df.copy()\n",
    "            \n",
    "            pred_label = ue_info[\"pred_label\"]\n",
    "            ue_col = ue_info[\"ue_col\"]\n",
    "\n",
    "            # Get error and UE cols\n",
    "            y_pred_cols = [col+pred_label+\"_\"+regressor_label for col in pred_cols]\n",
    "            y_pred = test_df[y_pred_cols].values\n",
    "            ue = test_df[ue_col].values\n",
    "            loss = np.mean(np.abs(y_true-y_pred), axis=1)\n",
    "            ue_df[\"loss\"] = loss\n",
    "\n",
    "            # Normalise UE\n",
    "            ue = min_max_norm_wo_outliers(ue)\n",
    "            ue_df[ue_col] = ue\n",
    "            max_range = round(max(ue), 1)\n",
    "            bin_edges = np.array([0]+list(np.linspace(bin_width, int(max_range*100), num=int(max_range*10))/100))\n",
    "            \n",
    "            # Plot scatter\n",
    "            scatter_ax = axes[0, ue_i]\n",
    "            scatter_ax.axvspan(0, bin_width/100, color='grey', alpha=0.2, linewidth=0)\n",
    "            scatter_ax.scatter(ue, loss, **formatting_dict)\n",
    "            # Plot line\n",
    "            m, c = np.polyfit(ue, loss, 1) \n",
    "            ue = np.sort(ue)\n",
    "            scatter_ax.plot(ue, m*ue+c, color=line_col, linestyle='-', label=f'y = {m:.3f}x + {c:.3f}', linewidth=1.5)\n",
    "            scatter_ax.legend(fontsize=eqn_label_fs)\n",
    "            if ue_i == 0:\n",
    "                axes[0, ue_i].set_ylabel(\"MAE\")\n",
    "                \n",
    "            # Plot Barplot\n",
    "            ue_df['bin'] = pd.cut(ue_df[ue_col], bins=bin_edges, labels=bin_edges[1:], include_lowest=True, right=True)\n",
    "            grouped = ue_df.groupby(\"bin\", observed=False)\n",
    "            grouped_loss = grouped[\"loss\"].mean()\n",
    "            # Comparison of losses\n",
    "            bar_ax = axes[1, ue_i]\n",
    "            bar_ax.bar(\n",
    "                bin_edges[1:]-bin_width/100/2, grouped_loss, width=bin_width/100*0.9, color=\"#0090C1\")\n",
    "            ticks = list(axes[1, ue_i].get_yticks(minor=True)) + [grouped_loss[0.1]]\n",
    "            bar_ax.set_yticks(ticks, minor=True)\n",
    "            if ue_i == 0:\n",
    "                bar_ax.set_ylabel(\"Mean MAE\")\n",
    "            bar_ax.set_xlabel(ue_type)\n",
    "\n",
    "    if save_fig:\n",
    "        fp_fig_folder = os.path.join(fp_project_model_evaluations, str(seed))\n",
    "        plt.savefig(os.path.join(fp_fig_folder, \"ue_error_scatter.jpg\"), bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8f0ae-16c8-43ae-b089-0521bc722bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_dict = {\n",
    "    \"RUE\": {\n",
    "        \"pred_label\": \"_direct\", \"ue_col\": \"rue\", \n",
    "    },\n",
    "    \"Infer-Noise\": {\n",
    "        \"pred_label\": \"_infernoise\", \"ue_col\": \"infernoise_uncertainty\", \n",
    "    },\n",
    "    \"MC Dropout\": {\n",
    "        \"pred_label\": \"_mean\", \"ue_col\": \"mcd\", \n",
    "    },\n",
    "    \"GPR\": {\n",
    "        \"pred_label\": \"_gpr\", \"ue_col\": \"gpr_std_mean\", \n",
    "    },\n",
    "    \"BNN\": {\n",
    "        \"pred_label\": \"_bnn\", \"ue_col\": \"bnn_uncertainty\", \n",
    "    },\n",
    "    \"DER\": {\n",
    "        \"pred_label\": \"_der\", \"ue_col\": \"der_uncertainty\", \n",
    "    },\n",
    "}    \n",
    "get_ue_loss_scatterplot(df_dict, ue_dict, seed=seed, dpi=300, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c401923-db01-462a-8ec2-d410964ccbdc",
   "metadata": {},
   "source": [
    "#### Starplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c224a-cba8-4503-8182-e4b8089c8197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_pi_metrics_starplot(pi_stats, categories=pi_stats.columns.tolist()[2:-2]):\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    from plotly.colors import hex_to_rgb\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=pi_stats[\"Time Horizon\"].nunique(),\n",
    "        specs=[[{\"type\": \"scatterpolar\"}, {\"type\": \"scatterpolar\"}, {\"type\": \"scatterpolar\"}]], \n",
    "        subplot_titles=pi_stats[\"Time Horizon\"].unique()\n",
    "    )\n",
    "    pi_stats = pi_stats.copy()\n",
    "    color_list = [\n",
    "        '#1f77b4',  # muted blue\n",
    "        '#ff7f0e',  # safety orange\n",
    "        '#2ca02c',  # cooked asparagus green\n",
    "        '#d62728',  # brick red\n",
    "        '#9467bd',  # muted purple\n",
    "        '#8c564b',  # chestnut brown\n",
    "        '#e377c2',  # raspberry yogurt pink\n",
    "    #     '#7f7f7f',  # middle gray\n",
    "    #     '#bcbd22',  # curry yellow-green\n",
    "    #     '#17becf'   # blue-teal\n",
    "    ]\n",
    "    \n",
    "    for i, (time_horizon_label, time_df) in enumerate(pi_stats.groupby(\"Time Horizon\")):\n",
    "        # time_df[categories] = (time_df[categories]-time_df[categories].min())/(time_df[categories].max()-time_df[categories].min())\n",
    "        time_df[categories] = time_df[categories]/time_df[categories].max()\n",
    "        # time_df[categories] = np.log(time_df[categories])\n",
    "        for j, (method_label, method_df) in enumerate(time_df.groupby(\"Method\")):\n",
    "            cur_col = list(hex_to_rgb(color_list[j]))\n",
    "            cur_fill_col = f'rgba({cur_col[0]}, {cur_col[1]}, {cur_col[2]}, 0.3)'\n",
    "            cur_outline_col = f'rgba({cur_col[0]}, {cur_col[1]}, {cur_col[2]}, 0.8)'\n",
    "            fig.append_trace(\n",
    "                go.Scatterpolar(\n",
    "                      r=method_df[categories].values[0],\n",
    "                      theta=categories,\n",
    "                      fill='toself',\n",
    "                      name=method_label if i==0 else None,\n",
    "                      fillcolor=cur_fill_col,\n",
    "                      marker=dict(color=cur_outline_col, line=dict(\n",
    "                            color=color_list[j],\n",
    "                            width=1\n",
    "                        ),\n",
    "                    ),\n",
    "                    showlegend=i==0\n",
    "                ), row=1, col=i+1\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "              visible=False,\n",
    "            )\n",
    "        ),\n",
    "        polar2=dict(\n",
    "            radialaxis=dict(\n",
    "              visible=False,\n",
    "            )\n",
    "          ),\n",
    "        polar3=dict(\n",
    "            radialaxis=dict(\n",
    "              visible=False,\n",
    "            )\n",
    "      ),\n",
    "      showlegend=True, \n",
    "      width=850,  \n",
    "      height=350,\n",
    "      title_x=0.5,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            xanchor=\"center\",\n",
    "            y=-0.5,\n",
    "            x=0.5,\n",
    "            entrywidth=180\n",
    "        )\n",
    "    )\n",
    "#     fig.show()\n",
    "    return fig\n",
    "        \n",
    "plot_pi_metrics_starplot(pi_stats, categories=pi_stats.columns.tolist()[3:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd607ed7-0c62-47fd-a076-c39142ae7f3f",
   "metadata": {},
   "source": [
    "## Repeat Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7b86e-bfcd-4f71-a9b4-fb28082fe726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_seed = 2023\n",
    "\n",
    "def train_rue_n_predictor(seed, data_dict, override_checkpoints, testing):\n",
    "    # Load Best HP\n",
    "    all_rue_decoder_best_hp = joblib.load(os.path.join(fp_tuning, str(hp_seed), \"all_rue_decoder_best_hp.joblib\"))\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            fp_model = os.path.join(fp_project_models, str(seed), f\"rue_{time_label}\")\n",
    "            # Check if fp_model exists\n",
    "            if override_checkpoints or not os.path.exists(fp_model):\n",
    "                # Get best hyperparameter\n",
    "                best_hp = all_rue_decoder_best_hp[time_label]\n",
    "                # Train model\n",
    "                ae_regressor = model_training(\n",
    "                    best_hp, predictors=predictors, pred_cols=time_info_dict[\"outputs\"], \n",
    "                    train_df=time_info_dict[\"train_df\"], valid_df = time_info_dict[\"valid_df\"], seed=seed,\n",
    "                    batch_size=batch_size, max_epochs=10000 if not testing else 1, verbose=1, patience=20\n",
    "                ) \n",
    "                # Save model\n",
    "                save_model(\n",
    "                    model=ae_regressor, name=f\"rue_{time_label}\", \n",
    "                    fp_checkpoints=os.path.join(fp_project_models, str(seed)), override=True)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Training\")\n",
    "\n",
    "def predict_rue_n_predictor(seed, data_dict, override_checkpoints, testing):\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            # Load model\n",
    "            ae_regressor = load_model(name=f\"rue_{time_label}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "            # Check if we have predicted on the validation set\n",
    "            fp_valid_df = os.path.join(fp_project_predictions, str(seed), f\"rue_valid_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_valid_df):\n",
    "                rue_valid_df = model_test_predictions(\n",
    "                    ae_regressor, df_train=time_info_dict[\"train_df\"], df_test=time_info_dict[\"valid_df\"], \n",
    "                    pred_cols=time_info_dict[\"outputs\"], predictors=predictors, \n",
    "                    regressor_label=\"_\"+time_label, pred_min=int(time_label[-1]), T=10 if not testing else 1, seed=seed\n",
    "                )\n",
    "                rue_valid_df.to_csv(fp_valid_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Validation Prediction\")\n",
    "                \n",
    "            # Check if we have predicted on the testing set \n",
    "            fp_test_df = os.path.join(fp_project_predictions, str(seed), f\"rue_test_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_test_df):\n",
    "                rue_test_df = model_test_predictions(\n",
    "                    ae_regressor, df_train=time_info_dict[\"train_df\"], df_test=time_info_dict[\"test_df\"], \n",
    "                    pred_cols=time_info_dict[\"outputs\"], predictors=predictors, \n",
    "                    regressor_label=\"_\"+time_label, pred_min=int(time_label[-1]), T=10 if not testing else 1, seed=seed\n",
    "                )\n",
    "                rue_test_df.to_csv(fp_test_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Testing Prediction\")\n",
    "            \n",
    "def train_gpr(seed, data_dict, override_checkpoints, testing):\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            fp_model = os.path.join(fp_project_models, str(seed), f\"gpr_{time_label[-1]}\")\n",
    "            # Check if fp_model exists\n",
    "            if override_checkpoints or not os.path.exists(fp_model):\n",
    "                # Train model\n",
    "                gpr = model_training_gpr( \n",
    "                    predictors=predictors, pred_cols=time_info_dict[\"outputs\"], \n",
    "                    train_df=time_info_dict[\"train_df\"], valid_df = time_info_dict[\"valid_df\"], seed=seed\n",
    "                ) \n",
    "                # Save model\n",
    "                save_model_gpr(model=gpr, name=f\"gpr_{time_label[-1]}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Training\")\n",
    "\n",
    "def predict_gpr(seed, data_dict, override_checkpoints, testing):\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            gpr = load_model_gpr(name=f\"gpr_{time_label[-1]}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "            # Check if we have predicted on the validation set\n",
    "            fp_valid_df = os.path.join(fp_project_predictions, str(seed), f\"gpr_valid_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_valid_df):\n",
    "                gpr_valid_df = model_test_predictions_gpr(\n",
    "                    gpr=gpr, df_test=time_info_dict[\"valid_df\"], pred_cols=time_info_dict[\"outputs\"], \n",
    "                    predictors=predictors, regressor_label=f\"_{time_label}\", pred_min=int(time_label[-1]), seed=seed)\n",
    "                gpr_valid_df.to_csv(fp_valid_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Validation Prediction\")\n",
    "                \n",
    "            # Check if we have predicted on the testing set\n",
    "            fp_test_df = os.path.join(fp_project_predictions, str(seed), f\"gpr_test_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_test_df):\n",
    "                gpr_test_df = model_test_predictions_gpr(\n",
    "                    gpr=gpr, df_test=time_info_dict[\"test_df\"], pred_cols=time_info_dict[\"outputs\"], \n",
    "                    predictors=predictors, regressor_label=f\"_{time_label}\", pred_min=int(time_label[-1]), seed=seed)\n",
    "                gpr_test_df.to_csv(fp_test_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Testing Prediction\")    \n",
    "                \n",
    "def predict_infernoise(seed, data_dict, override_checkpoints, testing):\n",
    "    # Load best hp for infernoise\n",
    "    infernoise_hp_dict = joblib.load(os.path.join(fp_tuning, str(hp_seed), \"all_infernoise_best_hp.joblib\"))\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            # Load model\n",
    "            ae_regressor = load_model(name=f\"rue_{time_label}\", fp_checkpoints=os.path.join(fp_project_models, str(seed)))\n",
    "            # Check if we have predicted on the validation set\n",
    "            fp_valid_df = os.path.join(fp_project_predictions, str(seed), f\"infernoise_valid_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_valid_df):\n",
    "                infernoise_valid_df = infernoise_test_predictions(\n",
    "                     ae_regressor, test_df=time_info_dict[\"valid_df\"], inputs=predictors, outputs=time_info_dict[\"outputs\"], regressor_label=\"_\"+time_label, \n",
    "                     seed=seed, T=10 if not testing else 1, stddev=infernoise_hp_dict[time_label]\n",
    "                ) \n",
    "                infernoise_valid_df.to_csv(fp_valid_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Validation Prediction\")\n",
    "                \n",
    "            # Check if we have predicted on the testing set\n",
    "            fp_test_df = os.path.join(fp_project_predictions, str(seed), f\"infernoise_test_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_test_df):\n",
    "                infernoise_test_df = infernoise_test_predictions(\n",
    "                     ae_regressor, test_df=time_info_dict[\"test_df\"], inputs=predictors, outputs=time_info_dict[\"outputs\"], regressor_label=\"_\"+time_label, \n",
    "                     seed=seed, T=10 if not testing else 1, stddev=infernoise_hp_dict[time_label]\n",
    "                ) \n",
    "                infernoise_test_df.to_csv(fp_test_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Testing Prediction\")    \n",
    "\n",
    "def train_bnn(seed, data_dict, override_checkpoints, testing):\n",
    "    # Load Best HP\n",
    "    all_bnn_best_hp = joblib.load(os.path.join(fp_tuning, str(hp_seed), \"all_bnn_best_hp.joblib\"))\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            fp_model = os.path.join(fp_project_models, str(seed), f\"bnn_{time_label}.pt\")\n",
    "            # Check if fp_model exists\n",
    "            if override_checkpoints or not os.path.exists(fp_model):\n",
    "                # Get best hyperparameter\n",
    "                bnn_best_hp = all_bnn_best_hp[time_label]\n",
    "                # Train model\n",
    "                bnn_model = train_model_w_best_param(\n",
    "                    best_param=bnn_best_hp, train_df=time_info_dict[\"train_df\"], valid_df=time_info_dict[\"valid_df\"], \n",
    "                    feat_cols=predictors, target_cols=time_info_dict[\"outputs\"],\n",
    "                    epochs=500 if not testing else 1, patience=5, seed=seed, fp_model=fp_model\n",
    "                )\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Training\")\n",
    "\n",
    "def predict_bnn(seed, data_dict, override_checkpoints, testing):\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            # Load model\n",
    "            fp_model = os.path.join(fp_project_models, str(seed), f\"bnn_{time_label}.pt\")\n",
    "            bnn_model = torch.load(fp_model)\n",
    "            # Check if we have predicted on the validation set\n",
    "            fp_valid_df = os.path.join(fp_project_predictions, str(seed), f\"bnn_valid_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_valid_df):\n",
    "                bnn_valid_df = bnn_model_prediction(\n",
    "                    bnn_model, time_info_dict[\"valid_df\"], \n",
    "                    feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], \n",
    "                    T=10 if not testing else 1, seed=seed, regressor_label=time_label)\n",
    "                bnn_valid_df.to_csv(fp_valid_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Validation Prediction\")\n",
    "                \n",
    "            # Check if we have predicted on the testing set \n",
    "            fp_test_df = os.path.join(fp_project_predictions, str(seed), f\"bnn_test_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_test_df):\n",
    "                bnn_test_df = bnn_model_prediction(\n",
    "                    bnn_model, time_info_dict[\"test_df\"], \n",
    "                    feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], \n",
    "                    T=10 if not testing else 1, seed=seed, regressor_label=time_label)\n",
    "                bnn_test_df.to_csv(fp_test_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Testing Prediction\")\n",
    "\n",
    "def train_der(seed, data_dict, override_checkpoints, testing):\n",
    "    # Load Best HP\n",
    "    all_der_best_hp = joblib.load(os.path.join(fp_tuning, str(hp_seed), \"all_der_best_hp.joblib\"))\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            fp_model = os.path.join(fp_project_models, str(seed), f\"der_{time_label}.pt\")\n",
    "            # Check if fp_model exists\n",
    "            if override_checkpoints or not os.path.exists(fp_model):\n",
    "                # Get best hyperparameter\n",
    "                best_hp = all_der_best_hp[time_label]\n",
    "                # Train model\n",
    "                der_model, _ = train_der_w_param(\n",
    "                    **best_hp, train_df=time_info_dict[\"train_df\"], valid_df=time_info_dict[\"valid_df\"], \n",
    "                    inputs=predictors, outputs=time_info_dict[\"outputs\"],\n",
    "                    seed=seed, max_epochs= 500 if not testing else 1, patience=5\n",
    "                )\n",
    "                torch.save(der_model, fp_model)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Training\")\n",
    "\n",
    "def predict_der(seed, data_dict, override_checkpoints, testing):\n",
    "    with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "        for time_label, time_info_dict in pbar:\n",
    "            pbar.set_description(time_label)\n",
    "            # Load model\n",
    "            fp_model = os.path.join(fp_project_models, str(seed), f\"der_{time_label}.pt\")\n",
    "            der_model = torch.load(fp_model)\n",
    "            # Check if we have predicted on the validation set\n",
    "            fp_valid_df = os.path.join(fp_project_predictions, str(seed), f\"der_valid_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_valid_df):\n",
    "                der_valid_df = der_model_prediction(\n",
    "                    der_model, test_df=time_info_dict[\"valid_df\"], \n",
    "                    feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], \n",
    "                    seed=seed, silent=False, regressor_label=time_label)\n",
    "                der_valid_df.to_csv(fp_valid_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Validation Prediction\")\n",
    "                \n",
    "            # Check if we have predicted on the testing set \n",
    "            fp_test_df = os.path.join(fp_project_predictions, str(seed), f\"der_test_{time_label[-1]}.csv\")\n",
    "            if override_checkpoints or not os.path.exists(fp_test_df):\n",
    "                der_test_df = der_model_prediction(\n",
    "                    der_model, test_df=time_info_dict[\"test_df\"], \n",
    "                    feat_cols=predictors, target_cols=time_info_dict[\"outputs\"], \n",
    "                    seed=seed, silent=False, regressor_label=time_label)\n",
    "                der_test_df.to_csv(fp_test_df)\n",
    "            else:\n",
    "                print(f\"- Skip {time_label} Testing Prediction\")\n",
    "                \n",
    "def save_df_dict_fast(df_dict, seed):\n",
    "    fp_df_dict = os.path.join(fp_project_pi_predictions, str(seed), \"df_dict.joblib\")\n",
    "    joblib.dump(df_dict, fp_df_dict)\n",
    "\n",
    "def load_df_dict_fast( seed):\n",
    "    fp_df_dict = os.path.join(fp_project_pi_predictions, str(seed), \"df_dict.joblib\")\n",
    "    return joblib.load(fp_df_dict)\n",
    "\n",
    "def retrieve_history_file(seed):\n",
    "    fp_history = os.path.join(fp_project_pi_predictions, str(seed), \"df_dict_history.joblib\")\n",
    "    if os.path.exists(fp_history):\n",
    "        return joblib.load(fp_history)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def add_to_pi_history_file(event, seed):\n",
    "    fp_history = os.path.join(fp_project_pi_predictions, str(seed), \"df_dict_history.joblib\")\n",
    "    if not os.path.exists(fp_history):\n",
    "        history = set(event)\n",
    "    else:\n",
    "        history = joblib.load(fp_history)\n",
    "        history.add(event)\n",
    "    joblib.dump(history, fp_history)\n",
    "    \n",
    "def create_df_dict(seed, data_dict, override_checkpoints, testing):\n",
    "    event = \"creation\"\n",
    "    history = retrieve_history_file(seed)\n",
    "    if (override_checkpoints) or (event not in history):\n",
    "        df_dict = {}\n",
    "        with tqdm(data_dict.items(), total=len(data_dict)) as pbar:\n",
    "            for time_label, time_info_dict in pbar:\n",
    "                df_dict[time_label] = {\n",
    "                    \"valid_df\": load_all_predictions(time_label=int(time_label[-1]), split=\"valid\", seed=seed),\n",
    "                    \"test_df\": load_all_predictions(time_label=int(time_label[-1]), split=\"test\", seed=seed),\n",
    "                    \"pred_cols\": time_info_dict[\"outputs\"]\n",
    "                }\n",
    "        save_df_dict_fast(df_dict, seed)\n",
    "        add_to_pi_history_file(event, seed)\n",
    "    else:\n",
    "        print(f\"- Skip df_dict creation!\")\n",
    "\n",
    "def generate_prediction_interval(seed, data_dict, override_checkpoints, testing, \n",
    "                                 event, pi_func, ue_dict):\n",
    "    history = retrieve_history_file(seed)\n",
    "    if (override_checkpoints) or (event not in history):\n",
    "        df_dict = load_df_dict_fast(seed)\n",
    "        scaler = load_scaler(fp_downsampled_scaler_file)\n",
    "        for time_label, time_info in df_dict.items():\n",
    "            val_df, test_df, pred_cols = time_info[\"valid_df\"], time_info[\"test_df\"], time_info[\"pred_cols\"]\n",
    "            for ue_label, ue_info in ue_dict.items():\n",
    "                pred_label, ue_col = ue_info[\"pred_label\"], ue_info[\"ue_col\"]\n",
    "                df_dict[time_label][\"test_df\"] = pi_func(\n",
    "                    df_val=df_dict[time_label][\"valid_df\"], df_test=df_dict[time_label][\"test_df\"], predictors=predictors, pred_cols=pred_cols, \n",
    "                    pred_label=pred_label, regressor_label=time_label, ue_col=ue_col, scaler=scaler, seed=seed\n",
    "                )\n",
    "        save_df_dict_fast(df_dict, seed)\n",
    "        add_to_pi_history_file(event, seed)\n",
    "    else:\n",
    "        print(f\"- Skip {event} PI!\")\n",
    "\n",
    "def generate_knn_pi(seed, data_dict, override_checkpoints, testing):\n",
    "    ue_dict = {\"RUE\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\"}} \n",
    "    generate_prediction_interval(\n",
    "        seed, data_dict, override_checkpoints, testing, \n",
    "        event=\"knn\", pi_func=knn_prediction_interval, ue_dict=ue_dict)\n",
    "\n",
    "def generate_weighted_pi(seed, data_dict, override_checkpoints, testing):\n",
    "    ue_dict = {\"RUE\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\"}} \n",
    "    generate_prediction_interval(\n",
    "        seed, data_dict, override_checkpoints, testing, \n",
    "        event=\"weighted\", pi_func=weighted_prediction_interval, ue_dict=ue_dict)\n",
    "\n",
    "def generate_conditional_pi(seed, data_dict, override_checkpoints, testing):\n",
    "    ue_dict = {\"RUE\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\"}} \n",
    "    generate_prediction_interval(\n",
    "        seed, data_dict, override_checkpoints, testing, \n",
    "        event=\"conditional\", pi_func=cond_gauss_prediction_interval, ue_dict=ue_dict)\n",
    "\n",
    "def generate_copula_pi(seed, data_dict, override_checkpoints, testing):\n",
    "    ue_dict = {\"RUE\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\"}} \n",
    "    generate_prediction_interval(\n",
    "        seed, data_dict, override_checkpoints, testing, \n",
    "        event=\"copula\", pi_func=gauss_copula_prediction_interval, ue_dict=ue_dict)\n",
    "\n",
    "def generate_conformal_pi(seed, data_dict, override_checkpoints, testing):\n",
    "    ue_dict = {\n",
    "        \"RUE\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\"},\n",
    "        \"MC Dropout\": {\"pred_label\": \"_mean\", \"ue_col\": \"mcd\", },\n",
    "        \"GPR\": {\"pred_label\": \"_gpr\", \"ue_col\": \"gpr_std_mean\", },\n",
    "        \"Infer-Noise\": {\"pred_label\": \"_infernoise\", \"ue_col\": \"infernoise_uncertainty\", },\n",
    "        \"BNN\": {\"pred_label\": \"_bnn\", \"ue_col\": \"bnn_uncertainty\", },\n",
    "        \"DER\": {\"pred_label\": \"_der\", \"ue_col\": \"der_uncertainty\", }\n",
    "    }   \n",
    "    generate_prediction_interval(\n",
    "        seed, data_dict, override_checkpoints, testing, \n",
    "        event=\"conformal\", pi_func=conformal_prediction_interval, ue_dict=ue_dict)\n",
    "\n",
    "def evaluate_model_perf(seed, data_dict, override_checkpoints, testing):\n",
    "    df_dict = load_df_dict_fast(seed)\n",
    "    ue_dict = {\n",
    "        \"RUE\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\"},\n",
    "        \"MC Dropout\": {\"pred_label\": \"_mean\", \"ue_col\": \"mcd\", },\n",
    "        \"GPR\": {\"pred_label\": \"_gpr\", \"ue_col\": \"gpr_std_mean\", },\n",
    "        \"Infer-Noise\": {\"pred_label\": \"_infernoise\", \"ue_col\": \"infernoise_uncertainty\", },\n",
    "        \"BNN\": {\"pred_label\": \"_bnn\", \"ue_col\": \"bnn_uncertainty\", },\n",
    "        \"DER\": {\"pred_label\": \"_der\", \"ue_col\": \"der_uncertainty\", }\n",
    "    }   \n",
    "    fp_pred_perf = os.path.join(fp_project_model_evaluations, str(seed), \"pred_perf.csv\")\n",
    "    if override_checkpoints or not os.path.exists(fp_pred_perf):\n",
    "        pred_perf_df = get_prediction_performance_table(df_dict, ue_dict)\n",
    "        display(pred_perf_df)\n",
    "        pred_perf_df.to_csv(fp_pred_perf)\n",
    "    else:\n",
    "        print(f\"- Skip prediction performance evaluation!\")\n",
    "\n",
    "def evaluate_ue_perf(seed, data_dict, override_checkpoints, testing):\n",
    "    df_dict = load_df_dict_fast(seed)\n",
    "    ue_dict = {\n",
    "        \"RUE\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\"},\n",
    "        \"MC Dropout\": {\"pred_label\": \"_mean\", \"ue_col\": \"mcd\", },\n",
    "        \"GPR\": {\"pred_label\": \"_gpr\", \"ue_col\": \"gpr_std_mean\", },\n",
    "        \"Infer-Noise\": {\"pred_label\": \"_infernoise\", \"ue_col\": \"infernoise_uncertainty\", },\n",
    "        \"BNN\": {\"pred_label\": \"_bnn\", \"ue_col\": \"bnn_uncertainty\", },\n",
    "        \"DER\": {\"pred_label\": \"_der\", \"ue_col\": \"der_uncertainty\", }\n",
    "    }   \n",
    "    fp_ue_perf = os.path.join(fp_project_model_evaluations, str(seed), \"ue_perf.csv\")\n",
    "    if override_checkpoints or not os.path.exists(fp_ue_perf):\n",
    "        ue_perf_df = get_ue_performance_table(df_dict, ue_dict)\n",
    "        display(ue_perf_df)\n",
    "        ue_perf_df.to_csv(fp_ue_perf)\n",
    "    else:\n",
    "        print(f\"- Skip UE performance evaluation!\")\n",
    "\n",
    "    fp_ue_scatter = os.path.join(fp_project_model_evaluations, str(seed), \"ue_error_scatter.jpg\")\n",
    "    if override_checkpoints or not os.path.exists(fp_ue_scatter):\n",
    "        get_ue_loss_scatterplot(df_dict, ue_dict, seed=seed, dpi=300, save_fig=True)\n",
    "    else:\n",
    "        print(f\"- Skip UE-Error Scatter Plot!\")\n",
    "\n",
    "def evaluate_pi_perf(seed, data_dict, override_checkpoints, testing):\n",
    "    override_checkpoints = True\n",
    "    df_dict = load_df_dict_fast(seed)\n",
    "    scaler = load_scaler(fp_downsampled_scaler_file)\n",
    "    pi_dict = {\n",
    "        \"RUE Gaussian Copula\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_gauss_copula\"},\n",
    "        \"RUE Conditional Gaussian\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_cond_gauss\"},\n",
    "        \"RUE Weighted\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_weighted\"},\n",
    "        \"RUE KNN\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_knn\"},\n",
    "        \"RUE Conformal\": {\"pred_label\": \"_direct\", \"ue_col\": \"rue\", \"pi_label\": \"_conformal\"},\n",
    "        \"Infer-Noise Conformal\": {\"pred_label\": \"_infernoise\", \"ue_col\": \"infernoise_uncertainty\", \"pi_label\": \"_conformal\"},\n",
    "        \"MC Dropout Conformal\": {\"pred_label\": \"_mean\", \"ue_col\": \"mcd\", \"pi_label\": \"_conformal\"},\n",
    "        \"GPR Conformal\": {\"pred_label\": \"_gpr\", \"ue_col\": \"gpr_std_mean\", \"pi_label\": \"_conformal\"},\n",
    "        \"BNN Conformal\": {\"pred_label\": \"_bnn\", \"ue_col\": \"bnn_uncertainty\", \"pi_label\": \"_conformal\"},\n",
    "        \"DER Conformal\": {\"pred_label\": \"_der\", \"ue_col\": \"der_uncertainty\", \"pi_label\": \"_conformal\"},\n",
    "    }   \n",
    "    fp_pi_perf = os.path.join(fp_project_model_evaluations, str(seed), \"pi_perf.csv\")\n",
    "    if override_checkpoints or not os.path.exists(fp_pi_perf):\n",
    "        pi_perf_df = calculate_metrics(df_dict, pi_dict)\n",
    "        display(pi_perf_df)\n",
    "        pi_perf_df.to_csv(fp_pi_perf)\n",
    "    else:\n",
    "        pi_perf_df = pd.read_csv(fp_pi_perf, index_col=0)\n",
    "        print(f\"- Skip PI performance evaluation!\")\n",
    "\n",
    "    fp_pi_folder = os.path.join(fp_project_model_evaluations, str(seed), \"pi_line_graphs\")\n",
    "    if override_checkpoints or not os.path.exists(fp_pi_folder):\n",
    "        plot_predictions_with_pi(df_dict, pi_dict, scaler, seed=seed, dpi=300, save_figs=True)\n",
    "    else:\n",
    "        print(f\"- Skip PI Line Plots!\")\n",
    "\n",
    "    fp_pi_parallel_coord = os.path.join(fp_project_model_evaluations, str(seed), \"parallel_coord.jpg\")\n",
    "    if override_checkpoints or not os.path.exists(fp_pi_parallel_coord):\n",
    "        plot_pi_metrics_parallel_coordinate(pi_perf_df, seed=seed, categories=[\"CP\", \"PINAW\", \"PINAFD\"], save_fig=True)\n",
    "    else:\n",
    "        print(f\"- Skip PI Parallel Coordinates Plot!\")\n",
    "    \n",
    "def run_one_experiment(seed, override_checkpoints=False, testing=False):\n",
    "    data_dict = {\n",
    "        \"t+1\": {\"train_df\": df_train_1, \"valid_df\": df_valid_1, \"test_df\": df_test_1, \"outputs\": pred_cols_1},\n",
    "        \"t+2\": {\"train_df\": df_train_2, \"valid_df\": df_valid_2, \"test_df\": df_test_2, \"outputs\": pred_cols_2},\n",
    "        \"t+3\": {\"train_df\": df_train_3, \"valid_df\": df_valid_3, \"test_df\": df_test_3, \"outputs\": pred_cols_3},\n",
    "    }\n",
    "    checkpoint_dict = {\n",
    "        \"Train RUE and Predictor\": train_rue_n_predictor,\n",
    "        \"Prediction with RUE and Predictor\": predict_rue_n_predictor,\n",
    "        \"Prediction with Infer-Noise\": predict_infernoise,\n",
    "        \"Train GPR\": train_gpr,\n",
    "        \"Prediction with GPR\": predict_gpr,\n",
    "        \"Train BNN\": train_bnn, \n",
    "        \"Prediction with BNN\": predict_bnn, \n",
    "        \"Train DER\": train_der, # <-\n",
    "        \"Prediction with DER\": predict_der,\n",
    "        \"Create DF Dict\": create_df_dict,\n",
    "        \"Generate KNN PI\":generate_knn_pi,\n",
    "        \"Generate Weighted PI\": generate_weighted_pi,\n",
    "        \"Generate Conditional PI\": generate_conditional_pi,\n",
    "        \"Generate Copula PI\": generate_copula_pi,\n",
    "        \"Generate Conformal PI\": generate_conformal_pi,\n",
    "        \"Evaluate Model Performance\": evaluate_model_perf,\n",
    "        \"Evaluate UE Performance\": evaluate_ue_perf,\n",
    "        \"Evaluate PI Performance\": evaluate_pi_perf\n",
    "    }\n",
    "    create_all_seed_folders(seed)\n",
    "    for i, (checkpoint_name, checkpoint_func) in enumerate(checkpoint_dict.items()):\n",
    "        print(f\"{i+1}. {checkpoint_name}\")\n",
    "        start = time.time()\n",
    "        checkpoint_func(seed, data_dict, override_checkpoints, testing)\n",
    "        print(f\"- {checkpoint_name} took {time.time()-start} s.\")\n",
    "\n",
    "def run_multiple_experiments(seed_list, override_checkpoints=False, testing=False):\n",
    "    with tqdm(seed_list, total=len(seed_list)) as pbar:\n",
    "        for seed in pbar:\n",
    "            pbar.set_description(str(seed))\n",
    "            run_one_experiment(seed, override_checkpoints, testing)\n",
    "\n",
    "start_seed = 2024\n",
    "num_experiments = 4\n",
    "seed_list = range(start_seed, start_seed+num_experiments)\n",
    "run_multiple_experiments(seed_list, override_checkpoints=False, testing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d38741-e888-41b7-8e87-dee3f3c486e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:21:11.694855Z",
     "iopub.status.busy": "2024-10-22T02:21:11.694498Z",
     "iopub.status.idle": "2024-10-22T02:21:11.698089Z",
     "shell.execute_reply": "2024-10-22T02:21:11.697594Z",
     "shell.execute_reply.started": "2024-10-22T02:21:11.694823Z"
    }
   },
   "source": [
    "### Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0555f5cf-a1ac-4df0-a274-845844d6ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list=list(range(2023, 2023+5))\n",
    "def combine_mean_n_std_matrices(mean, std):\n",
    "    assert mean.shape == std.shape\n",
    "    shape = mean.shape\n",
    "    returned_list = []\n",
    "    for i in range(shape[0]):\n",
    "        cur_list = []\n",
    "        for j in range(shape[1]):\n",
    "            cur_list.append(f\"{mean[i][j]:.3f}  {std[i][j]:.3f}\")\n",
    "        returned_list.append(cur_list)\n",
    "    return returned_list\n",
    "\n",
    "def get_average_of_all_seed_csvs(seed_list, fp_folder, filename, index_col=None):\n",
    "    result_list = []\n",
    "    for cur_seed in seed_list:\n",
    "        fp_perf = os.path.join(fp_folder, str(cur_seed), filename)\n",
    "        df = pd.read_csv(fp_perf, index_col=0)\n",
    "        if index_col:\n",
    "            df = df.set_index(index_col)\n",
    "        result_list.append(df.values)\n",
    "    results = np.array(result_list)\n",
    "    combined_mean = np.mean(results, axis=0)\n",
    "    combined_std = np.std(results, axis=0)\n",
    "    return pd.DataFrame(\n",
    "        combine_mean_n_std_matrices(combined_mean, combined_std), \n",
    "        index=df.index, columns=df.columns)\n",
    "\n",
    "def get_mean_of_all_seed_csvs(seed_list, fp_folder, filename, index_col=None):\n",
    "    result_list = []\n",
    "    for cur_seed in seed_list:\n",
    "        fp_perf = os.path.join(fp_folder, str(cur_seed), filename)\n",
    "        df = pd.read_csv(fp_perf, index_col=0)\n",
    "        if index_col:\n",
    "            df = df.set_index(index_col)\n",
    "        result_list.append(df.values)\n",
    "    results = np.array(result_list)\n",
    "    combined_mean = np.mean(results, axis=0)\n",
    "    return pd.DataFrame(\n",
    "        combined_mean, \n",
    "        index=df.index, columns=df.columns)\n",
    "\n",
    "best_color = \"#88E7B8\"\n",
    "second_best_color = \"#FAC05E\"\n",
    "\n",
    "# Function to highlight most and second highest\n",
    "def highlight_first_n_second_highest(s):\n",
    "    s = s.map(lambda x: float(x.split(\" \")[0]))\n",
    "    highest = s.nlargest(1).iloc[-1] # Find the highest value\n",
    "    second_highest = s.nlargest(2).iloc[-1]  # Find the second highest value\n",
    "    output = []\n",
    "    for v in s:\n",
    "        if v == highest:\n",
    "            output.append(f'background-color: {best_color}')\n",
    "        elif v == second_highest:\n",
    "            output.append(f'background-color: {second_best_color}')\n",
    "        else:\n",
    "            output.append(\"\")\n",
    "    return output\n",
    "\n",
    "def highlight_first_n_second_lowest(s):\n",
    "    s = s.map(lambda x: float(x.split(\" \")[0]))\n",
    "    smallest = s.nsmallest(1).iloc[-1] # Find the highest value\n",
    "    second_smallest = s.nsmallest(2).iloc[-1]  # Find the second highest value\n",
    "    output = []\n",
    "    for v in s:\n",
    "        if v == smallest:\n",
    "            output.append(f'background-color: {best_color}')\n",
    "        elif v == second_smallest:\n",
    "            output.append(f'background-color: {second_best_color}')\n",
    "        else:\n",
    "            output.append(\"\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26866e0d-99f4-4485-97b3-744fce577cce",
   "metadata": {},
   "source": [
    "#### Prediction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0479fa-41a6-40db-94df-44990bd262dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consolidated_pred_perf(seed_list):\n",
    "    return get_average_of_all_seed_csvs(seed_list, fp_project_model_evaluations, filename=\"pred_perf.csv\", index_col=\"Model\")\n",
    "\n",
    "def display_consolidated_pred_perf(pred_perf_df):\n",
    "    display(pred_perf_df.style.apply(highlight_first_n_second_lowest))\n",
    "\n",
    "pred_perf_df = get_consolidated_pred_perf(seed_list)\n",
    "# display(pred_perf_df)\n",
    "display_consolidated_pred_perf(pred_perf_df)\n",
    "pred_perf_df.to_csv(os.path.join(fp_project_consolidated_results, \"pred_perf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ed2d8-27b3-43f2-b164-e829e0caeedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to bold the best\n",
    "def bold_best(s, direction):\n",
    "    s = s.map(lambda x: float(x.replace(r\"\\underline{\", \"\").replace(\"}\", \"\").split(\" \")[0]))\n",
    "    if direction == \"max\":\n",
    "        best = s.nlargest(1).iloc[-1] # Find the best\n",
    "    elif direction == \"min\":\n",
    "        best = s.nsmallest(1).iloc[-1] # Find the best\n",
    "    else:\n",
    "        raise Exception(f\"Invalid direction {direction}!\")\n",
    "    \n",
    "    output = []\n",
    "    for v in s:\n",
    "        if v == best:\n",
    "            output.append(\"textbf:--rwrap;\")\n",
    "        else:\n",
    "            output.append(\"\")\n",
    "    return output\n",
    "\n",
    "# Function to underline the second best\n",
    "def underline_second_best(s, direction):\n",
    "    ori_s = s.copy()\n",
    "    s = s.map(lambda x: float(x.split(\" \")[0]))\n",
    "    if direction == \"max\":\n",
    "        second_best = s.nlargest(2).iloc[-1]  # Find the second best\n",
    "    elif direction == \"min\":\n",
    "        second_best = s.nsmallest(2).iloc[-1]  # Find the second best\n",
    "    else:\n",
    "        raise Exception(f\"Invalid direction {direction}!\")\n",
    "    \n",
    "    output = []\n",
    "    for ori_v, v in zip(ori_s, s):\n",
    "        if v == second_best:\n",
    "            output.append(r'\\underline{'+ori_v+'}')\n",
    "        else:\n",
    "            output.append(ori_v)\n",
    "    return output\n",
    "\n",
    "def df_to_latex(df, column_format_dict):\n",
    "    df = df.copy()\n",
    "    for col, direction in column_format_dict.items():\n",
    "        df[col] = underline_second_best(df[col], direction)\n",
    "    styler = df.style\n",
    "    # Bold column names\n",
    "    styler.map_index(lambda v: \"textbf:--rwrap;\", axis=\"columns\")\n",
    "    # Bold best\n",
    "    for col, direction in column_format_dict.items():\n",
    "        styler.apply(bold_best, subset=[col], direction=direction)\n",
    "    return styler.to_latex(column_format='c'*(df.shape[1]+df.index.nlevels))\n",
    "\n",
    "print(df_to_latex(pred_perf_df, column_format_dict={\"t+1\": \"min\", \"t+2\": \"min\", \"t+3\": \"min\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335d29a-5c4f-4b0c-963d-758aef43df2a",
   "metadata": {},
   "source": [
    "#### UE Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe762833-bb3f-4701-8ebf-80213ea3e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consolidated_ue_perf(seed_list):\n",
    "    return get_average_of_all_seed_csvs(seed_list, fp_project_model_evaluations, filename=\"ue_perf.csv\", index_col=\"Model\")\n",
    "\n",
    "ue_perf_df = get_consolidated_ue_perf(seed_list)\n",
    "display(ue_perf_df)\n",
    "ue_perf_df.to_csv(os.path.join(fp_project_consolidated_results, \"ue_perf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a3db1-361c-492a-be06-33913683e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_consolidated_ue_perf(ue_perf_df):\n",
    "    ue_perf_df = ue_perf_df.copy()\n",
    "    # Split df into time label\n",
    "    num_time, num_metrics = 3, 7\n",
    "    for i in range(num_time):\n",
    "        print(f\"t+{i}:\")\n",
    "        column_indices = list(range(i*num_metrics, (i+1)*num_metrics))\n",
    "        cur_df = ue_perf_df.iloc[:,column_indices]\n",
    "        cur_df.columns = cur_df.columns.str.split(\" \").str[-1] # remove time label from column names\n",
    "        display(\n",
    "            cur_df.style.apply(\n",
    "                highlight_first_n_second_highest, subset=cur_df.columns[0]).apply(\n",
    "                    highlight_first_n_second_lowest, subset=cur_df.columns[2:]\n",
    "                )\n",
    "        )\n",
    "        \n",
    "display_consolidated_ue_perf(ue_perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e836c-ba5a-4be8-a65f-bfdb11007d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to bold the best\n",
    "def bold_best_grouped(s, direction):\n",
    "    s = s.map(lambda x: float(x.replace(r\"\\underline{\", \"\").replace(\"}\", \"\").split(\" \")[0]))\n",
    "    if direction == \"max\":\n",
    "        best = s.nlargest(1).iloc[-1] # Find the best\n",
    "    elif direction == \"min\":\n",
    "        best = s.nsmallest(1).iloc[-1] # Find the best\n",
    "    else:\n",
    "        raise Exception(f\"Invalid direction {direction}!\")\n",
    "    \n",
    "    output = []\n",
    "    for v in s:\n",
    "        if v == best:\n",
    "            output.append(\"textbf:--rwrap;\")\n",
    "        else:\n",
    "            output.append(\"\")\n",
    "    return output\n",
    "\n",
    "# Function to underline the second best\n",
    "def bold_best_underline_second_best_grouped(s, direction, group_col):\n",
    "    s = s.copy()\n",
    "    output = []\n",
    "    for group_name, group_s in s.groupby(level=group_col):\n",
    "        ori_group_s = group_s.copy()\n",
    "        group_s = group_s.map(lambda x: float(x.split(\" \")[0]))\n",
    "        if direction == \"max\":\n",
    "            best = group_s.nlargest(1).iloc[-1] # Find the best\n",
    "            second_best = group_s.nlargest(2).iloc[-1]  # Find the second best\n",
    "        elif direction == \"min\":\n",
    "            best = group_s.nsmallest(1).iloc[-1] # Find the best\n",
    "            second_best = group_s.nsmallest(2).iloc[-1]  # Find the second best\n",
    "        else:\n",
    "            raise Exception(f\"Invalid direction {direction}!\")\n",
    "        for ori_v, v in zip(ori_group_s, group_s):\n",
    "            if v == best:\n",
    "                output.append(r'\\textbf{'+ori_v+'}')\n",
    "            elif v == second_best:\n",
    "                output.append(r'\\underline{'+ori_v+'}')\n",
    "            else:\n",
    "                output.append(ori_v)\n",
    "    return output\n",
    "\n",
    "def ue_perf_to_latex(ue_perf_df):\n",
    "    # TODO: Groupby + df to latex\n",
    "    ue_perf_df = ue_perf_df.copy()\n",
    "    \n",
    "    # Split df into time label\n",
    "    num_time, num_metrics = 3, 7\n",
    "    all_dfs = []\n",
    "    for i in range(num_time):\n",
    "        print()\n",
    "        column_indices = list(range(i*num_metrics, (i+1)*num_metrics))\n",
    "        cur_df = ue_perf_df.iloc[:,column_indices]\n",
    "        cur_df.columns = cur_df.columns.str.split(\" \").str[-1] # remove time label from column names\n",
    "        cur_df.loc[:,\"Time Horizon\"] = f\"t+{i}\"\n",
    "        all_dfs.append(cur_df)\n",
    "    all_dfs = pd.concat(all_dfs)\n",
    "    all_dfs = all_dfs.reset_index().set_index([\"Time Horizon\", \"Model\"])\n",
    "    column_format_dict={\"Corr\": \"max\", \"AURC\":\"min\", \"Sigma=0.1\": \"min\", \"Sigma=0.2\": \"min\", \"Sigma=0.3\": \"min\", \"Sigma=0.4\": \"min\"}\n",
    "    for col, direction in column_format_dict.items():\n",
    "        all_dfs[col] = bold_best_underline_second_best_grouped(all_dfs[col], direction , group_col=\"Time Horizon\")\n",
    "    \n",
    "    return all_dfs.to_latex(column_format='c'*(all_dfs.shape[1]+all_dfs.index.nlevels))\n",
    "print(ue_perf_to_latex(ue_perf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88e6c44-8163-48fc-9434-992d3a7aa29a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:29:58.967474Z",
     "iopub.status.busy": "2024-10-22T02:29:58.967062Z",
     "iopub.status.idle": "2024-10-22T02:29:58.970961Z",
     "shell.execute_reply": "2024-10-22T02:29:58.970267Z",
     "shell.execute_reply.started": "2024-10-22T02:29:58.967442Z"
    }
   },
   "source": [
    "#### PI Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983a272-b752-4610-b81d-9ad1775b0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consolidated_pi_perf(seed_list):\n",
    "    return get_average_of_all_seed_csvs(seed_list, fp_project_model_evaluations, filename=\"pi_perf.csv\", index_col=[\"Time Horizon\",\"Method\"])\n",
    "\n",
    "pi_perf_df = get_consolidated_pi_perf(seed_list)\n",
    "display(pi_perf_df)\n",
    "pi_perf_df.to_csv(os.path.join(fp_project_consolidated_results, \"pi_perf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff1f5f-6427-4b5b-8e61-f3422981671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pi_mean_metrics_parallel_coordinate(seed_list, categories=[\"CP\", \"PINAW\", \"PINAFD\"], save_fig=False):\n",
    "    from matplotlib import cm\n",
    "    from matplotlib.colors import rgb2hex\n",
    "    import seaborn as sns\n",
    "\n",
    "    pi_stats = get_mean_of_all_seed_csvs(seed_list, fp_project_model_evaluations, filename=\"pi_perf.csv\", index_col=[\"Time Horizon\",\"Method\"]).reset_index()\n",
    "    \n",
    "    pi_stats = pi_stats.copy().sort_values(by=[\"Time Horizon\", \"Method\"])\n",
    "    num_cat = len(categories)\n",
    "    \n",
    "    rue_colors = sns.color_palette('spring', n_colors=4)\n",
    "    other_colors = sns.color_palette('winter', n_colors=3)\n",
    "    color_list = sns.color_palette('rainbow', n_colors=pi_stats[\"Method\"].nunique())\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_cat, sharey=True, dpi=300, figsize=(num_cat*3, 3))\n",
    "    xticks = range(num_cat)\n",
    "    \n",
    "    for i, (time_horizon_label, time_df) in enumerate(pi_stats.groupby(\"Time Horizon\")):\n",
    "        time_df[categories] = (time_df[categories]-time_df[categories].min())/(time_df[categories].max()-time_df[categories].min())\n",
    "        for j, (method_label, method_df) in enumerate(time_df.groupby(\"Method\")):\n",
    "            # print(method_df[categories].values.flatten())\n",
    "            axes[i].plot(xticks, method_df[categories].values.flatten(), label=method_label if i==0 else None, linestyle=\"--\" if \"RUE\" not in method_label else \"-\")\n",
    "            axes[i].set_xticks(xticks, categories)\n",
    "        axes[i].set_title(time_horizon_label)\n",
    "\n",
    "    reorder=lambda hl,nc:(sum((lis[i::nc]for i in range(nc)),[])for lis in hl)\n",
    "    h_l = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(*reorder(h_l, 5), loc='upper center', bbox_to_anchor=(0.5, 0), ncol=5, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    if save_fig:\n",
    "        plt.savefig(os.path.join(fp_project_consolidated_results, \"parallel_coord.jpg\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75145112-23dd-4dee-af3f-6591f8836e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pi_mean_metrics_parallel_coordinate(seed_list, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc4cdf-7eb2-404b-bd84-60d20356d472",
   "metadata": {},
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94d5cd-e71c-4618-8a8b-6903c2b1968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_runtime():\n",
    "    categories = [\"Training\", \"Predicting\", \"PI\"]\n",
    "    time_df = pd.read_csv(fp_time_log, header=None)\n",
    "    time_df.columns = [\"Seed\", \"Description\", \"Time/s\"]\n",
    "    time_df = time_df.drop_duplicates(keep=\"last\")\n",
    "    time_series = time_df.groupby(by=[\"Description\"])[\"Time/s\"].mean()\n",
    "    category_values = [category for op in time_series.index for category in categories if category in op]\n",
    "    output_df = pd.DataFrame({\"Category\": category_values, \"Operation\":time_series.index, \"Time/s\": time_series.values})\n",
    "    return output_df.sort_values(by=[\"Category\", \"Time/s\"]).set_index([\"Category\", \"Operation\"])\n",
    "\n",
    "runtime_df = analyse_runtime()\n",
    "display(runtime_df)\n",
    "runtime_df.to_csv(os.path.join(fp_project_consolidated_results, \"runtime.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962d73c-a53f-4977-9211-218ab3182323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
